{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Pattern (IP groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssdeep\n",
    "import pickle,os,sys,gc\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "import time,datetime\n",
    "from itertools import chain\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import functools\n",
    "import itertools\n",
    "import operator\n",
    "from itertools import chain\n",
    "import geoip2.database\n",
    "from geoip2.errors import AddressNotFoundError\n",
    "\n",
    "sys.setrecursionlimit(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isp = '遠傳電信'\n",
    "date_li = ['20200106','20200107','20200108','20200109','20200110','20200111','20200112']\n",
    "protocols_need = proto_li = ['http','mysql','ftp','smb','smtp','imap','pop','rpc','ssh','telnet','sip']\n",
    "time = str(min(time_li))\n",
    "picture_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/'+isp+'/case_pictures/'\n",
    "file_name = \"_\".join(sorted(date_li))\n",
    "min_date = str(min(date_li))\n",
    "max_date = str(max(date_li))\n",
    "noise_path = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(min_date[:4])+'_'+str(min_date[4:6])+'_'+str(min_date[6:])+'/'+isp+'/case_pickles/noise_cluster.pkl'\n",
    "denoise_path = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(min_date[:4])+'_'+str(min_date[4:6])+'_'+str(min_date[6:])+'/'+isp+'/case_pickles/denoise_cluster.pkl'\n",
    "pickle_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(min_date[:4])+'_'+str(min_date[4:6])+'_'+str(min_date[6:])+'/'+isp+'/case_pickles/'\n",
    "thr_li= [0.1,0.5,0.9] \n",
    "city_reader = geoip2.database.Reader('/home/antslab/NAS1_RAID6/GeoIP2-DB/GeoIP2-City_20200526/GeoIP2-City.mmdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 新關聯方法\n",
    "* topology\n",
    "    * 我們會先找所給定期間的指定所有protocols之所有sessions與對應IPs\n",
    "    * 接下來會利用此段期間的各IP，去尋找這個IP在這段期間做的手法(攻擊樣態群集)\n",
    "    * 找出不同IP所橫跨對應的攻擊樣態群，計算jaccard相似度\n",
    "    * 將所採用相似手法(score>thr)的IP群聚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#第二次跑:\n",
    "all_df = pickle.load(open(pickle_dir+'clusterName_overview_denoise_df_'+str(min_date)+'_'+str(date_li[-1])+'.pkl','rb'))\n",
    "noise_clusters = pickle.load(open(noise_path,'rb'))\n",
    "denoise_clusters = pickle.load(open(denoise_path,'rb'))\n",
    "assert len(noise_clusters) + len(denoise_clusters) == len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_protocol(cluster_id):\n",
    "    '''\n",
    "    GOAL: 將cluster index轉換為protocol名稱\n",
    "    '''\n",
    "    return cluster_id.split(\"_\")[-1]\n",
    "def count_sessions_num(time_li):\n",
    "    '''\n",
    "    GOAL: 依據timestamp list計算每個cluster所具有的session數量\n",
    "    '''\n",
    "    return len(time_li)\n",
    "all_df['protocol'] = all_df.idx.map(cut_protocol)\n",
    "all_df['session_num'] = all_df.timestamp.map(count_sessions_num)\n",
    "print(isp,\"經濾除noiseClusters後在\",min_date,\"至\",max_date,\"的期間中各protocols的session數量:\")\n",
    "temp = pd.DataFrame(all_df.groupby('protocol')['session_num'].sum()).sort_values('session_num',ascending=False)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all ips\n",
    "all_ips = all_df.src_ip.tolist()\n",
    "all_ips = sum(all_ips,[])\n",
    "all_ips = list(set(all_ips))\n",
    "all_ips = sorted(all_ips)\n",
    "col_li = all_df.idx.tolist()\n",
    "jc_matrix = pd.DataFrame(0, index=all_ips, columns=col_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in tqdm(jc_matrix.columns.tolist()):\n",
    "    if 'tds' in col: # 統一tds欄位，如果有的話\n",
    "        select_df = all_df[all_df.idx.str.contains('tds')]\n",
    "        ip_li = list(select_df.src_ip.values)\n",
    "        try:\n",
    "            for ips in ip_li:\n",
    "                jc_matrix.loc[ips,'tds'] = 1\n",
    "        except IndexError:\n",
    "            print(\"Didn't load tds protocol to all_ip. SKIPPING!\")\n",
    "            pass\n",
    "    else:\n",
    "        select_df = all_df[all_df.idx == col]\n",
    "        ip_li = list(select_df.src_ip.values)\n",
    "        for ips in ip_li:      \n",
    "            jc_matrix.loc[ips,col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jc_matrix_stat = jc_matrix.append(pd.Series(jc_matrix.sum(),name='stat'))\n",
    "jc_matrix_stat['np_array'] = list(jc_matrix_stat.values)\n",
    "def sum_arr(npy):\n",
    "    return sum(npy)\n",
    "jc_matrix_stat['sum'] = jc_matrix_stat.np_array.apply(sum_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc_matrix_stat = jc_matrix_stat.drop(['np_array'],axis=1)\n",
    "# pickle_dir = '/home/antslab/spark_data/pcap_inter/2020_01_06/中華電信/case_pickles/'\n",
    "pickle.dump(file=open(pickle_dir+'clusters_ips_stat_df_'+str(min_date)+'_'+str(max_date)+'.pkl','wb'),obj=jc_matrix_stat)\n",
    "print('One hot統計df:',pickle_dir+'clusters_ips_stat_df_'+str(min_date)+'_'+str(max_date)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#濾除col:\n",
    "jc_matrix_new = jc_matrix.append(pd.Series(jc_matrix.sum(),name='stat'))\n",
    "col_need = []\n",
    "for col in jc_matrix_new.columns:\n",
    "    if col == 'np_array':\n",
    "        continue\n",
    "    if jc_matrix_new.loc['stat',col] > 1:\n",
    "        col_need.append(col)\n",
    "jc_matrix_new = jc_matrix_new[col_need]\n",
    "#濾除row:\n",
    "jc_matrix_new['np_array'] = list(jc_matrix_new.values)#.ravel()\n",
    "jc_matrix_new['sum'] = jc_matrix_new.np_array.apply(sum_arr)\n",
    "jc_matrix_new = jc_matrix_new[jc_matrix_new['sum']>1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jc_matrix = jc_matrix_new.iloc[0:len(jc_matrix_new)-1]\n",
    "jc_matrix = jc_matrix.sort_values(['sum'],ascending=False)\n",
    "jc_matrix = jc_matrix.drop(['sum','np_array'],axis=1)\n",
    "jc_matrix['np_array'] = list(jc_matrix.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sum_val(npy):\n",
    "    return sum(npy)\n",
    "jc_matrix['sum'] = jc_matrix['np_array'].apply(sum_val)\n",
    "temp = jc_matrix[jc_matrix['sum'] == 0]\n",
    "weird_ips = temp.index.tolist()\n",
    "assert len(weird_ips) == 0 #檢查!! 不能有assertion err!!!\n",
    "jc_matrix = jc_matrix.drop(['sum'],axis=1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jc_matrix_stat = jc_matrix.append(pd.Series(jc_matrix.sum(),name='stat'))\n",
    "def sum_arr(npy):\n",
    "    return sum(npy)\n",
    "jc_matrix_stat['sum'] = jc_matrix_stat.np_array.apply(sum_arr)\n",
    "jc_matrix_stat = jc_matrix_stat.drop(['np_array'],axis=1)\n",
    "pickle.dump(file=open(pickle_dir+'clusters_ips_stat_afterFilter_df_'+str(min_date)+'_'+str(max_date)+'.pkl','wb'),obj=jc_matrix_stat)\n",
    "print(\"把col=1,row=1以下的濾掉之統計df:\",pickle_dir+'clusters_ips_stat_afterFilter_df_'+str(min_date)+'_'+str(max_date)+'.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_jac(c_value,t_value):\n",
    "    '''\n",
    "    GOAL: 同時考量jaccrd計算方式，與人類直覺計算方式\n",
    "    '''\n",
    "    j_s = jaccard_score(c_value, t_value)\n",
    "    c_s = cosine_similarity([c_value], [t_value])[0][0]\n",
    "    one_portion = max(sum(c_value),sum(t_value))/len(t_value) #最大長度的1的數量\n",
    "    final_score = (c_s*one_portion)+(j_s*(1-one_portion))\n",
    "    return final_score\n",
    "\n",
    "def calc_cos(c_value,t_value):\n",
    "    return cosine_similarity(c_value, t_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 不同thr都會需要重跑一次\n",
    "gc.collect()\n",
    "for thr in tqdm(thr_li):\n",
    "    jc_dict = {}\n",
    "    ip_li = jc_matrix.index.tolist() #pandas\n",
    "    used_ip = []\n",
    "    for ip in ip_li:\n",
    "        if ip in used_ip: #合併過得拿掉 single label\n",
    "            continue\n",
    "        t_value = jc_matrix.loc[ip,'np_array']# pandas\n",
    "        jc_calc = jc_matrix[~jc_matrix.index.isin(used_ip)] #合併過得拿掉 single label\n",
    "        jc_calc = jc_calc[jc_calc.index != ip] #自己的不比 singleLabel\n",
    "        jc_calc['jc_score'] = jc_calc.np_array.apply(calc_jac,args=(t_value,)) #得到t跟每個c的分數\n",
    "        combine_df = jc_calc[jc_calc['jc_score']>thr] #所設定的相似度分數\n",
    "        c_ips_li = combine_df.index.tolist() #跟這個IP具高度相似度的IPs\n",
    "        if len(c_ips_li)>0:\n",
    "            jc_dict[ip] = c_ips_li\n",
    "            used_ip.extend(c_ips_li) #合併過的不要再比\n",
    "            used_ip.append(ip) #用過的不要再比\n",
    "\n",
    "    loner_ip = list(set(ip_li) - set(used_ip))\n",
    "    min_date = str(min(date_li))\n",
    "    pickle_path = pickle_dir+'CorrelateIP_APTIP_thr'+str(thr)+'_'+file_name+'.pkl' #改!!\n",
    "    pickle.dump(obj=(jc_dict,loner_ip),file=open(pickle_path,'wb'))\n",
    "    print(\"threshold =\",thr,\"(jaccard_dictionary,loner_ip) save path:\",pickle_path)\n",
    "    print('集團數量(IP>1,score>'+str(thr)+'):',len(jc_dict),\"LonerIP數量:\",len(loner_ip))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(myList,target ):\n",
    "    return [i for i,j in enumerate(myList) if j == target]\n",
    "def find_time(indexes,li):\n",
    "    '''\n",
    "    一個IP回傳一個time list\n",
    "    '''\n",
    "    return list(map(li.__getitem__, indexes))\n",
    "def find_country(need_index,candidate_li):\n",
    "    '''\n",
    "    一個IP只回傳一個country\n",
    "    '''\n",
    "    return candidate_li[need_index[0]]\n",
    "def repeat_idx(ori_li,index):\n",
    "    return [index]*len(ori_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jc_matrix2 = jc_matrix.drop(['np_array'],axis=1)\n",
    "ori_col = all_df.idx.tolist()\n",
    "tds_ori_col = []\n",
    "for col in ori_col:\n",
    "    if 'tds' in col:\n",
    "        tds_ori_col.append(col)\n",
    "def find2(t_ip): #同個target ip\n",
    "    global temp\n",
    "    global temp2\n",
    "    global t_idx\n",
    "    temp = jc_matrix2[jc_matrix2.index == t_ip]\n",
    "    t_idx = temp.columns[temp.eq(1).any()]\n",
    "\n",
    "    temp2 = all_df[all_df.idx.isin(t_idx)]\n",
    "    temp2['gen'] = temp2.src_ip.apply(find,args=(t_ip,))\n",
    "    temp2['time_li'] = temp2.apply(lambda x: find_time(x.gen, x.timestamp), axis=1)\n",
    "    temp2['idx_li'] = temp2.apply(lambda x: repeat_idx(x.time_li, x.idx), axis=1)\n",
    "    temp2['country'] =  temp2.apply(lambda x: find_country(x.gen, x.country), axis=1)\n",
    "    return functools.reduce(operator.iconcat, temp2['time_li'].tolist(), []),functools.reduce(operator.iconcat, temp2['idx_li'].tolist(), []), temp2['country'].iloc[0] \n",
    "\n",
    "ip_li = jc_matrix.index.tolist()\n",
    "ip_df = pd.DataFrame(ip_li,columns=['src_ip'])\n",
    "ip_df['session_timelist'],ip_df['session_idlist'],ip_df['session_county'] = zip(*ip_df['src_ip'].apply(find2))\n",
    "pickle.dump(obj=ip_df,file=open(pickle_dir+'CorrelateIP_ALL_ip_df.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE\n",
    "* 可直接跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ip_df = pickle.load(open(pickle_dir+'CorrelateIP_ALL_ip_df.pkl','rb')) #check point\n",
    "\n",
    "for thr in tqdm(thr_li):\n",
    "    pickle_path = pickle_dir+'CorrelateIP_APTIP_thr'+str(thr)+'_'+file_name+'.pkl' #pickle路徑\n",
    "    jc_dict,loner_ip = pickle.load(open(pickle_path,'rb'))\n",
    "\n",
    "    similarity_id_list = []\n",
    "    timelist_dict_list = []\n",
    "    clusterlist_dict_list = []\n",
    "    country_list = []\n",
    "\n",
    "    for cluster_id, ip_li in jc_dict.items():\n",
    "        all_ips = ip_li[:]\n",
    "        all_ips.append(cluster_id)\n",
    "        temp = ip_df[ip_df.src_ip.isin(all_ips)]\n",
    "        temp_time = temp.set_index('src_ip')['session_timelist'].to_dict()\n",
    "        temp_id = temp.set_index('src_ip')['session_idlist'].to_dict()\n",
    "    #     temp_country = temp.set_index('src_ip')['country'].to_dict()\n",
    "        temp_country = temp['session_county'].tolist()\n",
    "        similarity_id_list.append(cluster_id) #僅識別用\n",
    "        timelist_dict_list.append(temp_time)\n",
    "        clusterlist_dict_list.append(temp_id)\n",
    "        country_list.append(temp_country)\n",
    "    pattern_select_df = pd.DataFrame([similarity_id_list,timelist_dict_list,clusterlist_dict_list,country_list],\n",
    "                 index=['pattern_key','sessions_time_dict','cluster_id_dict','country_list']).T\n",
    "    save_path = pickle_dir+'CorrelateIP_DRAW_'+str(thr)+'.pkl'\n",
    "    pickle.dump(obj=pattern_select_df,file=open(save_path,'wb'))\n",
    "    print(\"集團數量(IP>1,score>\"+str(thr)+\"):\",len(jc_dict),\"LonerIP數量:\",len(loner_ip))\n",
    "    print('視覺化路徑:',save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#每個IP有多少個session?\n",
    "global ip_session_count \n",
    "ip_session_count = {}\n",
    "def find_sessions_number(ip_li):\n",
    "    '''\n",
    "    INPUT: list\n",
    "    \n",
    "    '''\n",
    "    global ip_session_count\n",
    "    for ip in ip_li:\n",
    "        try:\n",
    "            val = ip_session_count[ip]\n",
    "            ip_session_count[ip] = val+1\n",
    "        except:\n",
    "            ip_session_count[ip] = 1\n",
    "all_df.src_ip.apply(find_sessions_number)\n",
    "ip_session_df = pd.DataFrame(ip_session_count.items())\n",
    "print(\"Total sessions#:\",ip_session_df[1].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#有m個session(key)的IP有幾個(value)\n",
    "ip_draw = dict(Counter(ip_session_df[1].tolist()))\n",
    "ip_draw = dict(sorted(ip_draw.items()))\n",
    "file_name = pickle_dir+'ip_sessions_statistics'+str(thr)+'.pkl' #改!!\n",
    "pickle.dump(file=open(file_name,'wb'),obj=ip_draw)\n",
    "print(\"有m個session(key)的IP有幾個(value) SAVE IN:\",file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loner ip 所對應的 cluster name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loner_cluster_dict = {}\n",
    "jc_matrix2 = jc_matrix.drop(['np_array'],axis='columns')\n",
    "for ip in loner_ip:\n",
    "    temp = pd.DataFrame(jc_matrix2.loc[ip])\n",
    "    temp = temp[temp[ip]==1]\n",
    "    cluster_name_li = temp.index.tolist()\n",
    "    loner_cluster_dict[ip] = cluster_name_li\n",
    "print(\"LonerIP共涵蓋\",len(loner_cluster_dict),\"個clusters\")\n",
    " \n",
    "loner_cluster_df = pd.DataFrame(loner_cluster_dict.items())\n",
    "loner_cluster_df[2] = loner_cluster_df[1].map(len)\n",
    "loner_cluster_df.columns = ['src_ip','cluster_name','cluster_num']\n",
    "file_name = pickle_dir+'lonerip_clusterName_df'+str(thr)+'.pkl' #改!!\n",
    "pickle.dump(file=open(file_name,'wb'),obj=loner_cluster_df)\n",
    "print(\"loner ip 所對應的 cluster name df SAVE IN:\",file_name)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算國家、IP數量、proto數量、cluster數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 需要各Cluster所對應之mitre轉換!\n",
    "cluster_name_dict = pickle.load(open('/home/antslab/NAS2_RAID5/pcap_inter/2020_01_06/中華電信/case_pickles/intention_dict_0106_0112.pkl','rb'))\n",
    "print(cluster_name_dict.keys())\n",
    "intention_dict = {}\n",
    "for in_name,cluster_li in cluster_name_dict.items():\n",
    "    for cluster in cluster_li:\n",
    "        intention_dict[cluster] = in_name\n",
    "# print(set(intention_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def count_ip(di):\n",
    "    '''\n",
    "    GOAL: count ip number\n",
    "    '''\n",
    "    return len(di)\n",
    "def count_cluster(di):\n",
    "    '''\n",
    "    GOAL: count unique clusters #\n",
    "    '''\n",
    "    li =  list(di.values())\n",
    "    return len(set(functools.reduce(operator.iconcat, li, [])))\n",
    "def proto_li(di):\n",
    "    '''\n",
    "    GOAL: extract protocols names\n",
    "    Return: unique list\n",
    "    '''\n",
    "    li = list(di.values())\n",
    "    cluster_li = list(set(functools.reduce(operator.iconcat, li, [])))\n",
    "    proto_li = [x.split('_')[-1] for x in cluster_li]\n",
    "    return sorted(set(proto_li))\n",
    "def country_li(li):\n",
    "    lis = list(set(li))\n",
    "    lis = [str(x) for x in lis]\n",
    "    return sorted(lis)\n",
    "def cluster_li(di):\n",
    "    '''\n",
    "    GOAL: extract clusters names\n",
    "    Return: unique list\n",
    "    '''\n",
    "    li = list(di.values())\n",
    "    cluster_li = list(set(functools.reduce(operator.iconcat, li, [])))\n",
    "    return sorted(set(cluster_li))\n",
    "def country_count(li):\n",
    "    '''\n",
    "    GOAL: count countries in the group's num\n",
    "    '''\n",
    "    count_dict = dict(Counter(li))\n",
    "    return {k: v for k, v in sorted(count_dict.items(), key=lambda item: item[1],reverse=True)}\n",
    "def country_portion(di):\n",
    "    '''\n",
    "    GOAL: count country's port ion the group\n",
    "    '''\n",
    "    all_nums = sum(list(di.values()))\n",
    "    df = pd.DataFrame(di.items())\n",
    "    df[1] = df[1]/all_nums\n",
    "    return df.set_index(0)[1].to_dict()\n",
    "def main_country(di):\n",
    "    '''\n",
    "    GOAL: return main country\n",
    "    '''\n",
    "    return list(di.keys())[0]\n",
    "def cluster_number(di):\n",
    "    '''\n",
    "    GOAL: count cluster number in each group\n",
    "    '''\n",
    "    li =  list(di.values())\n",
    "    count_dict = dict(Counter(list(functools.reduce(operator.iconcat, li, []))))\n",
    "    return {k: v for k, v in sorted(count_dict.items(), key=lambda item: item[1],reverse=True)}\n",
    "def cluster_portion(di):\n",
    "    '''\n",
    "    GOAL: use cluster_num to calculate cluster % in each group\n",
    "    '''\n",
    "    all_nums = sum(list(di.values()))\n",
    "    df = pd.DataFrame(di.items())\n",
    "    df[1] = df[1]/all_nums\n",
    "    return df.set_index(0)[1].to_dict()\n",
    "def intention_number(tmp_di):\n",
    "    '''\n",
    "    GOAL: transfer cluster name to intention categories.\n",
    "    '''\n",
    "    intention_num = {}\n",
    "    for c_name,c_num in tmp_di.items():\n",
    "        try:\n",
    "            i_name = intention_dict[c_name]\n",
    "        except KeyError:\n",
    "            i_name = 'probing'\n",
    "        try:\n",
    "            ori_num = intention_num[i_name]\n",
    "            intention_num[i_name] = ori_num + int(c_num)\n",
    "        except KeyError:\n",
    "            intention_num[i_name] = int(c_num)\n",
    "    return {k: v for k, v in sorted(intention_num.items(), key=lambda item: item[1],reverse=True)}\n",
    "def intention_portion(di):\n",
    "    '''\n",
    "    GOAL: calculate intention category's portion in dict type.\n",
    "    '''\n",
    "    all_nums = sum(list(di.values()))\n",
    "    df = pd.DataFrame(di.items())\n",
    "    df[1] = df[1]/all_nums\n",
    "    return df.set_index(0)[1].to_dict()    \n",
    "\n",
    "pattern_select_df['country_set'] = pattern_select_df.country_list.map(country_li)\n",
    "pattern_select_df['country_nums'] = pattern_select_df['country_list'].map(country_count)\n",
    "pattern_select_df['country_portion'] = pattern_select_df['country_nums'].map(country_portion)\n",
    "pattern_select_df['main_country'] = pattern_select_df['country_nums'].map(main_country)\n",
    "pattern_select_df['proto_set'] = pattern_select_df.cluster_id_dict.map(proto_li)\n",
    "pattern_select_df['cluster_set'] = pattern_select_df.cluster_id_dict.map(cluster_li)\n",
    "pattern_select_df['cluster_nums'] = pattern_select_df.cluster_id_dict.map(cluster_number)\n",
    "pattern_select_df['cluster_portion'] = pattern_select_df.cluster_nums.map(cluster_portion)\n",
    "pattern_select_df['intention_nums'] = pattern_select_df.cluster_nums.map(intention_number)\n",
    "pattern_select_df['intention_portion'] = pattern_select_df.intention_nums.map(intention_portion)\n",
    "pattern_select_df['ip_num'] = pattern_select_df.cluster_id_dict.map(count_ip)\n",
    "pattern_select_df['unique_country_num'] = pattern_select_df.country_set.map(count_ip)\n",
    "pattern_select_df['unique_cluster_num'] = pattern_select_df.cluster_id_dict.map(count_cluster)\n",
    "pattern_select_df['unique_protocols_num'] = pattern_select_df.proto_set.map(count_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = pickle_dir+'CorrelateIP_DRAW_stat'+str(thr)+'.pkl'\n",
    "pickle.dump(obj=pattern_select_df,file=open(save_path,'wb'))\n",
    "print('IP群統計做圖用df路徑:',save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loner ip's country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loner_country_info = ip_df[ip_df.src_ip.isin(loner_ip)]\n",
    "loner_country_info = loner_country_info.reset_index(drop=True)\n",
    "\n",
    "pickle.dump(file=open(pickle_dir+'loner_draw_country'+str(thr)+'.pkl','wb'),obj=loner_country_info)\n",
    "print(\"loner df資訊(可畫圖):\",pickle_dir+'loner_draw_country'+str(thr)+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手法(clusters)出現在哪些group、出現次數頻率\n",
    "* all_df搭配pattern_select_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names_li = all_df.idx.tolist()\n",
    "all_clusters = pattern_select_df['cluster_set'].tolist()\n",
    "all_clusters = list(functools.reduce(operator.iconcat, all_clusters, []))\n",
    "all_clusters = dict(Counter(all_clusters))\n",
    "all_clusters = {k: v for k, v in sorted(all_clusters.items(), key=lambda item: item[1],reverse=True)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(obj=all_clusters,file=open(pickle_dir+'clusterINgroup_stat_'+str(thr)+'_'+str(min_date)+'_'+str(max_date)+'.pkl','wb')) #改\n",
    "print(\"手法(clusters)出現次數頻率:\",pickle_dir+'clusterINgroup_stat_'+str(min_date)+'_'+str(max_date)+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同的IP會做哪些事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_country(ip):\n",
    "    return ip_df[ip_df['src_ip'] == ip]['session_county'].iloc[0]\n",
    "jc_matrix3 = jc_matrix2.reset_index()\n",
    "jc_matrix3['country'] = jc_matrix3['index'].map(find_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc_matrix_country = jc_matrix3.groupby('country').sum()\n",
    "jc_matrix_country_final = jc_matrix3.groupby('country').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc_matrix_country = jc_matrix3.groupby('country').sum()\n",
    "jc_matrix_country_final = jc_matrix3.groupby('country').sum()\n",
    "jc_matrix_country_final['max_behavior'] = jc_matrix_country.idxmax(axis=1)\n",
    "s = pd.Series(jc_matrix_country.idxmax(axis=0), name=\"max_country\")\n",
    "jc_matrix_country_final = jc_matrix_country_final.append(s)\n",
    "pickle.dump(file=open(pickle_dir+'country_behavior_table_'+str(min_date)+'_'+str(max_date)+'.pkl','wb'),obj=jc_matrix_country_final)\n",
    "print('Country Cluster df save in:',pickle_dir+'country_behavior_table_'+str(min_date)+'_'+str(max_date)+'.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將patern select df合併經緯度、抓出ssdeep hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need intention dict!\n",
    "def convert2intention(cluster_name):\n",
    "    try:\n",
    "        intention = intention_dict[cluster_name]\n",
    "    except KeyError:\n",
    "        intention = 'probing'\n",
    "    return intention\n",
    "all_df['intention'] = all_df.idx.apply(convert2intention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "city_reader_response = dict()\n",
    "def find_lalo_all(ip_li):\n",
    "    ip_df = pd.DataFrame(ip_li,columns=['ip'])\n",
    "    def find_lalo(ip):\n",
    "        try:\n",
    "            city_response = city_reader.city(ip)\n",
    "            latitude = city_response.location.latitude\n",
    "            longitude = city_response.location.longitude\n",
    "            return (latitude,longitude)\n",
    "        except (AddressNotFoundError,NameError):\n",
    "            return ('None','None')\n",
    "    ip_df['lalo'] = ip_df['ip'].apply(find_lalo)\n",
    "    return ip_df['lalo'].tolist()\n",
    "all_df['lalo'] = all_df.src_ip.map(find_lalo_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(file=open(pickle_dir+str(min_date)+'_'+str(max_date)+'_clusterID_time_country_ip_ssdeep.pkl','wb')\n",
    "            ,obj=all_df)\n",
    "print(\"Cluster資訊df path:\",\n",
    "      pickle_dir+str(min_date)+'_'+str(max_date)+'_clusterID_time_country_ip_ssdeep_lalo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Need intention dict!\n",
    "def need_col(gb):\n",
    "    d = {}\n",
    "    country_li = gb['country'].tolist() #抓出group by後的country欄位，並把所有值轉換為list\n",
    "    lalo_li = gb['lalo'].tolist()\n",
    "    time_li = gb['timestamp'].tolist()\n",
    "    d['country'] = list(functools.reduce(operator.iconcat, country_li, [])) #合併所有list為一個list\n",
    "    d['lalo'] = list(functools.reduce(operator.iconcat, lalo_li, []))\n",
    "    d['timestamp'] = list(functools.reduce(operator.iconcat, time_li, []))\n",
    "    return pd.Series(d,index=['country','lalo','timestamp'])\n",
    "def sort_li(time_li, country_li):\n",
    "    '''\n",
    "    GOAL: sort by time (align with time's order)\n",
    "    Return: list\n",
    "    '''\n",
    "    sort_country_li = [x for _,x in sorted(zip(time_li,country_li))]\n",
    "    return sort_country_li\n",
    "# Deprecated! Need Fix!!#Need Fix intention dict\n",
    "draw_intention_df = all_df.groupby('intention').apply(need_col)\n",
    "draw_intention_df['country'] = draw_intention_df.apply(lambda x:sort_li(x.timestamp,x.country),axis=1)\n",
    "draw_intention_df['lalo'] = draw_intention_df.apply(lambda x:sort_li(x.timestamp,x.lalo),axis=1)\n",
    "draw_intention_df['timestamp'] = draw_intention_df.timestamp.map(sorted)\n",
    "pickle.dump(file=open(pickle_dir+str(min_date)+'_'+str(max_date)+'_intention_country_lalo_drawdf.pkl','wb'),obj=draw_intention_df)\n",
    "print(\"全球視覺化地圖df path:\",\n",
    "      pickle_dir+str(min_date)+'_'+str(max_date)+'_intention_country_lalo_drawdf.pkl')\n",
    "draw_intention_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
