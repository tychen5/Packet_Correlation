{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssdeep\n",
    "import pickle,os,sys,gc\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "import subprocess\n",
    "from multiprocessing import Pool \n",
    "import time,datetime\n",
    "from itertools import chain\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import functools\n",
    "import itertools\n",
    "import operator\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import os.path\n",
    "from os import path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antslab/anaconda3/bin/python\n",
      "3.7.9 (default, Aug 31 2020, 12:42:55) \n",
      "[GCC 7.3.0]\n",
      "sys.version_info(major=3, minor=7, micro=9, releaselevel='final', serial=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0 3.0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://slave-59:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[12]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7feeb3615090>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  import sys,os\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "import findspark\n",
    "# os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3' #'/home/antslab/anaconda3/bin/python'\n",
    "findspark.init()\n",
    "import pyspark\n",
    "# from pyspark import SparkContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row, SQLContext, SparkSession, window\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "import  pyspark.sql.functions as F\n",
    "conf = SparkConf()\n",
    "# conf.set(\"spark.local.dir\", \"/mnt/ssd240g/data/Leo_Spark_Home/tmp\")\n",
    "conf.set(\"spark.executor.cores\",\"12\")\n",
    "conf.setMaster(\"local[12]\")\n",
    "conf.set(\"spark.driver.memory\",\"260g\") #滿載165G/\n",
    "conf.set(\"spark.executor.memory\", \"95g\")\n",
    "conf.set(\"spark.memory.offHeap.enabled\",\"true\")\n",
    "conf.set(\"spark.memory.offHeap.size\",\"85g\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\",\"-Xss80g\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\",\"-Xms80g\")\n",
    "# conf.set(\"spark.memory.storageFraction\",\"0.2\")\n",
    "# conf.set(\"spark.driver.maxResultSize\",\"0\")\n",
    "# conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "# conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "conf.set(\"spark.kubernetes.pyspark.pythonVersion\",\"3\")\n",
    "conf.set(\"spark.network.timeout\",\"1000s\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\",100000)\n",
    "conf.set(\"spark.driver.maxResultSize\", \"150g\")\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"pcap_analyzer\").config(conf=conf).getOrCreate() #pcap_analyzer pcapAnalyzer\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Test_spark\").config(conf=conf).getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 290000)\n",
    "spark.conf.set(\"spark.sql.hive.filesourcePartitionFileCacheSize\",100*1024*1024*1024)\n",
    "\n",
    "import databricks.koalas as ks\n",
    "ks.options.display.max_rows = 20\n",
    "ks.set_option('compute.max_rows', None)\n",
    "# ks.set_option('compute.ops_on_diff_frames', True)\n",
    "ks.set_option('compute.default_index_type', 'distributed')\n",
    "print(ks.__version__,spark.version)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isp = 'mi' #All\n",
    "\n",
    "in_file = np.nan\n",
    "pickle_dir = np.nan\n",
    "picture_dir = np.nan\n",
    "df2 = \"\"\n",
    "def create_orifinalDF(time):\n",
    "    \"\"\"\n",
    "    GOAL: preprocess original df\n",
    "    time: e.g., 20200101\n",
    "    \n",
    "    Return: df2\n",
    "    \"\"\"\n",
    "    \n",
    "    global in_file,pickle_dir,picture_dir #in file HDFS?\n",
    "    in_file = 'hdfs://192.168.50.123:8020/user/hdfs/parquet/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/'+isp+'/session_parquet/'+str(time)+'_session.parquet'\n",
    "#     in_file = 'hdfs://192.168.50.200/user/hdfs/parquet/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/'+isp+'/session_parquet/'+str(time)+'_session.parquet'\n",
    "#     in_file = '/home/antslab/data_hdd4t/pcap_process/pcap_spark/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/中華電信/parquet/2020_01_10_session_v7-2_hashpayload_time_size.parquet'  \n",
    "    pickle_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/'+isp+'/case_pickles/'\n",
    "    picture_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/'+isp+'/case_pictures/'\n",
    "    \n",
    "    if not os.path.exists(pickle_dir):\n",
    "        os.makedirs(pickle_dir,exist_ok=True)\n",
    "    if not os.path.exists(picture_dir):\n",
    "        os.makedirs(picture_dir,exist_ok=True)    \n",
    "    df = ks.read_parquet(in_file)\n",
    "#     print(df.columns)\n",
    "    df = df[['session_time','session_time_list','session_duration', 'session_i_tt_packet', 'session_o_tt_packet',\n",
    "                 'session_i_tt_frame_length','session_o_tt_frame_length', 'udp_i_tt_length',\n",
    "                 'udp_o_tt_length', 'udp_i_avg_length', 'udp_o_avg_length','icmp_i_avg_length', 'icmp_o_avg_length',\n",
    "                 'icmp_i_tt_datagram_length','icmp_o_tt_datagram_length',\n",
    "                 'tcp_i_tt_payload_length','tcp_o_tt_payload_length', 'tcp_i_avg_payload_length',\n",
    "                 'tcp_o_avg_payload_length','ip_src', 'ip_dst','tcp_srcport', 'tcp_dstport', 'country','isp',\n",
    "                 'domain', 'frame_i_max_protocols','frame_o_max_protocols', 'tcp_i_payload_list', 'tcp_o_payload_list'        \n",
    "                ]]\n",
    "     #篩選出有in bound payload的session\n",
    "    df1 = df[(df.session_duration>0.1)&(df.session_i_tt_packet>1)&(df.session_i_tt_frame_length>0)&\n",
    "             (df.tcp_i_payload_list.astype(str)!='[]')]\n",
    "    df1 = df1[(df1.udp_i_tt_length>0)|(df1.udp_i_avg_length>0)|(df1.icmp_i_avg_length>0)|\n",
    "            (df1.icmp_i_tt_datagram_length>0)|(df1.tcp_i_tt_payload_length>0)|(df1.tcp_i_avg_payload_length>0)]\n",
    "\n",
    "    df2 = df1[(df1.tcp_o_payload_list.astype(str)!='[]')&(df1.session_o_tt_packet>0)&(df1.session_o_tt_frame_length>0)]\n",
    "    df2 = df2[(df2.udp_o_tt_length>0)|(df2.udp_o_avg_length>0)|(df2.icmp_o_avg_length>0)|\n",
    "            (df2.icmp_o_tt_datagram_length>0)|(df2.tcp_o_tt_payload_length>0)|(df2.tcp_o_avg_payload_length>0)]\n",
    "    df2 = df2[(df2.domain != 'googlebot.com')|(df2.isp!='Googlebot')] #\n",
    "    gc.collect()\n",
    "    df2 = df2.to_pandas() #記憶體可能會不夠?\n",
    "#     print(time,\"=> 所有protocol總和中，1.原本的session總數:\",len(df),\" 2.in-bound具有payload的session總數:\",len(df1),\" 3.in-與out-bound都具有payload的session總數(最終拿來進行分群的sessions):\",len(df2))\n",
    "\n",
    "    #     print(df2.frame_i_max_protocols.value_counts().head(60))\n",
    "#     print(df2.frame_o_max_protocols.value_counts().head(60))\n",
    "#     print(df2.domain.value_counts().head(60))\n",
    "#     print(df2.isp.value_counts().head(60))\n",
    "#     protocol_name = 'tds'\n",
    "#     df2_tds = df2[(df2.frame_i_max_protocols.str.contains(protocol_name))&(df2.frame_o_max_protocols.str.contains(protocol_name))]\n",
    "#     print(time,'TDS sessions數量:',len(df2_tds),'TDS占所有protocols之sessions比例:',len(df2_tds)/len(df))\n",
    "    return df2,pickle_dir,picture_dir#,df2_tds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#記得改&\n",
    "def prepare_df(df2,protocol_name='http',split=\"size\",loner=False):\n",
    "    protocol_name = str(protocol_name).lower()\n",
    "    '''\n",
    "    df2: filterout no payload's all sessions df\n",
    "    protocol_name: what TCP protocol string would be contained e.g., smb,http,tds...\n",
    "    loner: 用來檢查不同天相同protocol的分群\n",
    "    \n",
    "    return1: protocol original dataframe (session-based)\n",
    "    return2: protocol payload dataframe (packet-based)\n",
    "    '''\n",
    "    def sort_fn(data):\n",
    "        '''\n",
    "        sort by time: itemgetter(1)\n",
    "        sort by size: itemgetter(2)\n",
    "        '''\n",
    "        if split=='size':\n",
    "            return sorted(data,key=itemgetter(2))\n",
    "        elif split == 'time':\n",
    "            return sorted(data,key=itemgetter(1))\n",
    "                    \n",
    "    def split_col_len(session):\n",
    "        '''\n",
    "        input: list(session) of lists(packets) =>tuple=(hash,time)\n",
    "\n",
    "        Return1: list of ssdeep length\n",
    "        Return2: list of session's packets hashes\n",
    "        '''\n",
    "    #     for session in payload_li:\n",
    "#         packet_payload = []\n",
    "        packet_len = []\n",
    "        for packet in session:\n",
    "            ssdeep_hash = packet[0]\n",
    "    #         packet_payload.append(ssdeep_hash)\n",
    "            packet_len.append(ssdeep_hash.split(':')[0])\n",
    "        return packet_len#,packet_payload\n",
    "\n",
    "    def split_col_hash(session):\n",
    "        '''\n",
    "        input: list(session) of lists(packets) =>tuple=(hash,time)\n",
    "\n",
    "        Return1: list of ssdeep length\n",
    "        Return2: list of session's packets hashes\n",
    "        '''\n",
    "    #     for session in payload_li:\n",
    "        packet_payload = []\n",
    "#         packet_len = []\n",
    "        for packet in session:\n",
    "            ssdeep_hash = packet[0]\n",
    "            packet_payload.append(ssdeep_hash)\n",
    "    #         packet_len.append(ssdeep_hash.split(':')[0])\n",
    "        return packet_payload\n",
    "    if not loner: ##改!!\n",
    "#         df2_protocol = df2[df2.frame_o_max_protocols.str.contains(protocol_name)]\n",
    "        df2_protocol = df2[(df2.frame_i_max_protocols.str.contains(protocol_name))&(df2.frame_o_max_protocols.str.contains(protocol_name))]\n",
    "    else:\n",
    "        df2_protocol = df2\n",
    "    df2_protocol_payload = df2_protocol[['tcp_i_payload_list']]\n",
    "    df2_protocol_payload['tcp_i_payload_list'] = df2_protocol_payload.tcp_i_payload_list.apply(sort_fn) #map\n",
    "    df2_protocol_payload['size'] = df2_protocol_payload.tcp_i_payload_list.map(split_col_len) #,df2_protocol_payload['hash']\n",
    "    df2_protocol_payload['hash'] = df2_protocol_payload.tcp_i_payload_list.map(split_col_hash)\n",
    "    L_size = [x if isinstance(x, list) else [x] for x in df2_protocol_payload['size']]\n",
    "    L_hash = [x if isinstance(x, list) else [x] for x in df2_protocol_payload['hash']]\n",
    "    df2_protocol_payload = pd.DataFrame({\n",
    "        'idx':df2_protocol_payload.index.values.repeat([len(x) for x in L_size]),\n",
    "        'size':list(chain.from_iterable(L_size)),\n",
    "        'hash':list(chain.from_iterable(L_hash))\n",
    "        })\n",
    "    # df2_protocol_payload['idx'] = \n",
    "    df2_protocol_payload['size'] = df2_protocol_payload['size'].astype(int)\n",
    "    if not loner:\n",
    "        return df2_protocol, df2_protocol_payload\n",
    "    else:\n",
    "        return df2_protocol_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_compare(df2_protocol,df2_protocol_payload,ori_protocol=None,ori_protocol_payload=None,thr=0,thr2=10,loner=False): #40\n",
    "    '''\n",
    "    GOAL: compare with timestamp's q1,q2,q3 to similar size packet. Calculate max/mean value for score.\n",
    "    df2_protocol: protocol original dataframe (session-based)\n",
    "    df2_protocol_payload: protocol payload dataframe (packet-based)\n",
    "    loner:需指派ori_protocol、ori_protocol_payload\n",
    "    \n",
    "    Return: dictionary with each cluster \n",
    "    '''\n",
    "    def ssdeep_compare(target_hash,candidate_df):\n",
    "        '''\n",
    "        Input1: string of hash\n",
    "        Input2: dataframe of candidate\n",
    "        '''\n",
    "        def compare(candidate_hash):\n",
    "            return ssdeep.compare(target_hash,candidate_hash)\n",
    "        return candidate_df.hash.map(compare)\n",
    "    big_dict_protocol = {}\n",
    "    big_dict_protocol_score = {}\n",
    "#     big_dict_protocol2 = {} #test\n",
    "    used_idx_li = []\n",
    "    all_scores = [] #test2\n",
    "    for idx in df2_protocol.index:\n",
    "        if idx in used_idx_li:\n",
    "            continue\n",
    "        target = df2_protocol_payload[df2_protocol_payload.idx == idx]\n",
    "        target = target.reset_index(drop=True)\n",
    "        t_len = len(target)\n",
    "        q1 = int(t_len*0.25)\n",
    "        q2 = int(t_len*0.5)\n",
    "        t_q1 = target.iloc[q1,-1]\n",
    "        if not loner:\n",
    "            candidate = df2_protocol_payload[df2_protocol_payload.idx!=idx]\n",
    "            candidate = candidate[~candidate.idx.isin(used_idx_li)] #candidat不可重複分群\n",
    "        else:\n",
    "            t_ip = df2_protocol[df2_protocol.index == idx].ip_src.values[0]\n",
    "            candidate_idx = ori_protocol[ori_protocol.ip_src == t_ip].index.tolist()\n",
    "            candidate = ori_protocol_payload[ori_protocol_payload.idx.isin(candidate_idx)] #可重複分群?\n",
    "        \n",
    "        candidate['idx'] = candidate.idx.astype(str)\n",
    "#         candidate_q1 = candidate[candidate['size']==target.iloc[q1,-2]] #rule\n",
    "        candidate['q1'] = ssdeep_compare(t_q1,candidate) #rule:candidate_q1\n",
    "        if q2 != q1:\n",
    "            t_q2 = target.iloc[q2,-1]\n",
    "            q3 = int(t_len*0.75)\n",
    "#             candidate_q2 = candidate[candidate['size']==target.iloc[q2,-2]] #rule\n",
    "            candidate['q2'] = ssdeep_compare(t_q2,candidate) #rule:candidate_q2\n",
    "            if q3 != q2:\n",
    "                t_q3 = target.iloc[q3,-1]\n",
    "#                 candidate_q3 = candidate[candidate['size']==target.iloc[q3,-2]] #rule\n",
    "                candidate['q3'] = ssdeep_compare(t_q3,candidate) #rule:candidate_q3\n",
    "                if t_len>4:\n",
    "#                     t_min = target.iloc[0,-1] \n",
    "#                     candidate['min'] = ssdeep_compare(t_min,candidate)\n",
    "                    t_max = target.iloc[-1,-1]\n",
    "                    candidate['max'] = ssdeep_compare(t_max,candidate)\n",
    "                    gc.collect()\n",
    "                    \n",
    "        candidate = candidate.drop(['size','hash'],axis=1)\n",
    "        candidate['score'] = candidate.mean(axis=1) #max\n",
    "        score_li = candidate.score.tolist() #test2\n",
    "        candidate = candidate[candidate.score>thr] #相似度分數，數字越小速度愈快、數字越大越多群\n",
    "        candidate['idx'] = candidate.idx.astype(int)\n",
    "#         candidate2 = candidate[candidate.score<thr2] #test\n",
    "        idx_li = list(set(candidate.idx.tolist()))\n",
    "#         idx_li2 = list(set(candidate2.idx.tolist())) #test\n",
    "        all_scores.extend(score_li) #test2\n",
    "        if len(idx_li)>0:\n",
    "            \n",
    "            used_idx_li.extend(idx_li)\n",
    "            used_idx_li.append(idx)\n",
    "            if not loner:\n",
    "                big_dict_protocol[idx] = idx_li\n",
    "                big_dict_protocol_score[idx] = candidate.score.tolist()\n",
    "            else:\n",
    "                big_dict_protocol[t_ip+'_'+str(idx)] = idx_li\n",
    "                big_dict_protocol_score[t_ip+'_'+str(idx)] = candidate.score.tolist()                \n",
    "#         if len(idx_li2)>0: #test\n",
    "#             big_dict_protocol2[idx] = idx_li2 # test\n",
    "    return big_dict_protocol,sorted(list(set(df2_protocol.index)-set(used_idx_li))),all_scores,big_dict_protocol_score #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write2pkl(df2_protocol,big_dict,cluster_key,case_id,path=pickle_dir):\n",
    "    '''\n",
    "    GOAL: output 3-tuple to draw time diagram\n",
    "    df2_protocol: original dataframe session-based\n",
    "    big_dict: big_dict_protocol\n",
    "    cluster_key: print by find_cluster function\n",
    "    case_id: user defined to identify\n",
    "    '''\n",
    "    cluster_key = int(cluster_key)\n",
    "#     print(big_dict[cluster_key])\n",
    "    temp_li = big_dict[cluster_key][:]\n",
    "#     print(temp_li)\n",
    "    temp_li.append(cluster_key) #最後一個session才是key (target)，其他人(candidates)都是跟他(target)比\n",
    "#     print(temp_li)\n",
    "#     temp_li = temp_li2[:]\n",
    "    case_diagram = df2_protocol.loc[list(set(temp_li))]\n",
    "    time_lists = case_diagram.session_time_list.tolist()\n",
    "    time_lists = [list(x) for x in time_lists]\n",
    "    ips = case_diagram.ip_src.tolist()\n",
    "    countries = case_diagram.country.tolist()\n",
    "#     print('Unique Country#:',len(set(countries)),\"Unique IP#:\",len(set(ips)),'Session#:',len(time_lists))\n",
    "    # pickle_dir = '/home/antslab/spark_data/pcap_inter/2020_01_10/中華電信/case_pickles/'\n",
    "    pickle.dump(obj=(time_lists,ips,countries),file=open(path+'case#'+str(case_id)+'_threetuples.pkl','wb'))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster(big_dict,idx):\n",
    "    '''\n",
    "    big_dict: big_dict_protocol\n",
    "    idx: which idx want to find\n",
    "    Return cluster id (big_dict\"s key number')\n",
    "    '''\n",
    "    idx = int(idx)\n",
    "    try:\n",
    "        big_dict[idx] #KeyError\n",
    "        return idx\n",
    "    except KeyError:\n",
    "        for k,v in big_dict.items():\n",
    "            if idx in v:\n",
    "                return k\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_stat(protocol_big_dict,protocol_df,proto,date,picture_dir,drawlog=False):\n",
    "    \"\"\"\n",
    "    GOAL: output statistic of certain protocol\n",
    "    input1: dict from similarity_compare\n",
    "    inpit2: df from prepare_df\n",
    "    \"\"\"\n",
    "    \n",
    "    all_nums = []\n",
    "    clus_id = []\n",
    "    # all_num = 0\n",
    "    for k,v in protocol_big_dict.items():\n",
    "    #     all_nums.extend(v)\n",
    "        all_nums.append(len(v))\n",
    "        clus_id.append(k)\n",
    "    #     all_num +=len(v)\n",
    "    clus_num = len(all_nums)\n",
    "    print(\"原本總共sessions#:\",len(protocol_df))\n",
    "    print(\"共有#sessions可分群:\",sum(all_nums)+clus_num,\"共有#lonerSessions:\",len(protocol_df)-(sum(all_nums)+clus_num))\n",
    "    print(\"可分為#群:\",clus_num)\n",
    "#     print(sum(all_nums)+len(all_nums),len(protocol_df)-(sum(all_nums)+len(all_nums)),len(protocol_df))\n",
    "    protocol_stat_df = pd.DataFrame(all_nums,index=clus_id)\n",
    "    protocol_stat_df[0] = protocol_stat_df[0]+1\n",
    "    print(\"前五大的cluster key與對應之群集大小\\n\",protocol_stat_df[0].nlargest(5))\n",
    "    print(protocol_stat_df.describe())\n",
    "    protocol_stat_df  = pd.DataFrame(all_nums,index=[x for x in range(len(all_nums))])\n",
    "    protocol_stat_df.hist(bins=100)\n",
    "    plt.savefig(picture_dir+str(proto)+'_'+str(date)+'_clustersize_hist'+'.png', dpi=300, format='png')\n",
    "    \n",
    "    temp = dict(Counter(all_nums))\n",
    "    temp = {k: v for k, v in sorted(temp.items(), key=lambda item: item[0])}\n",
    "#     temp = sorted(all_nums,reverse=True)\n",
    "    x_axis = [k for k in temp.keys()] #群中所含的數量\n",
    "    y_axis = [v for v in temp.values()] #該數量共有幾群為此\n",
    "    plt.figure(figsize=(30,20),dpi=300,linewidth = 1) # 圖片大小、折線寬度\n",
    "    plt.plot(x_axis,y_axis,'o-',color = 'b', label=proto) #折現的型態、折現的顏色\n",
    "#     plt.title(, x=0.5, y=1.03)\n",
    "    plt.figtext(.5,.9,str(date)+\" \"+str(proto)+\" cluster# of cluster's size\",fontsize=30,ha='center')\n",
    "#     plt.xticks(fontsize=20)\n",
    "#     plt.yticks(fontsize=20)\n",
    "    plt.xlabel(\"cluster size\", fontsize=25, labelpad = 15)\n",
    "    plt.ylabel(\"cluster #\", fontsize=25, labelpad = 20)\n",
    "    # plt.ylim(0, 2500)\n",
    "    # plt.legend(loc = \"best\", fontsize=20)\n",
    "    plt.savefig(picture_dir+str(proto)+'_'+str(date)+'_clusternumber_line'+'.png', dpi=300, format='png')\n",
    "#     plt.show()    \n",
    "    \n",
    "#log diagram\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_yscale('log')\n",
    "    protocol_stat_df.hist(ax=ax, bins=100, bottom=0.1)\n",
    "    plt.savefig(picture_dir+str(proto)+'_'+str(date)+'_clustersize_hist_log'+'.png', dpi=300, format='png')\n",
    "    \n",
    "    return len(protocol_df)-(sum(all_nums)+clus_num),protocol_stat_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_rep_hash(proto_big_dict,proto_df_payload,proto_df,proto_loners,knee_point,date='0110',protocol='http'):\n",
    "    '''\n",
    "    GOAL: create each cluster's representaion ssdeep hash\n",
    "    proto_df_payload: from prepare_df() function\n",
    "    proto_big_dict: from similarity_compare() function\n",
    "    proto_loners: loner idx list\n",
    "    knee_point: from get_small_cluster() function\n",
    "    date & protocol: user_defined cluster name\n",
    "    \n",
    "    Return: dict[cluster_name]: ssdeep hash\n",
    "    '''\n",
    "    upgma_dict = {}\n",
    "    for key,value in proto_big_dict.items(): #cluster rep ssdeep\n",
    "        target = proto_df_payload[proto_df_payload.idx == key]\n",
    "        target = target.reset_index(drop=True)\n",
    "        t_len = len(target)\n",
    "        q2 = int(t_len*0.5)\n",
    "        q2_hash = target.iloc[q2,-1]\n",
    "        country = proto_df.loc[key,'country']\n",
    "        ip = proto_df.loc[key,'ip_src']\n",
    "        domain = proto_df.loc[key,'domain']\n",
    "        if len(value)>knee_point:\n",
    "            upgma_dict[str(protocol)+'_'+str(key)+'_'+str(date)+'_'+str(country)+'_'+str(domain)+'_'+str(ip)] = q2_hash\n",
    "        else:\n",
    "            upgma_dict[str(protocol)+'_S_'+str(key)+'_'+str(date)+'_'+str(country)+'_'+str(domain)+'_'+str(ip)] = q2_hash\n",
    "    for key in proto_loners: #loner ssdeep\n",
    "        target = proto_df_payload[proto_df_payload.idx == key]\n",
    "        target = target.reset_index(drop=True)\n",
    "        t_len = len(target)\n",
    "        q2 = int(t_len*0.5)\n",
    "        q2_hash = target.iloc[q2,-1]\n",
    "        country = proto_df.loc[key,'country']\n",
    "        ip = proto_df.loc[key,'ip_src']\n",
    "        domain = proto_df.loc[key,'domain']\n",
    "        upgma_dict[str(protocol)+'_L_'+str(key)+'_'+str(date)+'_'+str(country)+'_'+str(domain)+'_'+str(ip)] = q2_hash\n",
    "    return upgma_dict\n",
    "\n",
    "\n",
    "\n",
    "def pair_wise_score(upgma_dict):\n",
    "    '''\n",
    "    GOAL: calculate distance matrix by calculating paire-wise similarity score. \n",
    "    and pick upper triangle convert to list\n",
    "    Input: from cluster_rep_hash() function\n",
    "    ['cluster']: name\n",
    "    ['ssdeep']: cluster's representation hash\n",
    "    \n",
    "    Return: df=>['c_ssdeep_li']:the hashes list compare to, ['score']:list of distances (upper-triangle, exclude self)\n",
    "    '''\n",
    "    def compare(target_hash,candidate_hash_li):\n",
    "        score_li = []\n",
    "        for c_hash in candidate_hash_li:\n",
    "            score_li.append(100-ssdeep.compare(target_hash,c_hash)) #相似度滿分100，轉換成距離最近0\n",
    "        return score_li\n",
    "    used_idx = []\n",
    "    def create_hash_li(t_hash):\n",
    "        idx_set = set(upgma_df[upgma_df.ssdeep == t_hash].index)#[0]\n",
    "        same_hash_li = sorted(list(idx_set - set(used_idx)))\n",
    "        idx = same_hash_li[0]\n",
    "        used_idx.append(idx)\n",
    "        return upgma_df.loc[idx+1:]['ssdeep'].tolist()\n",
    "    upgma_df = pd.DataFrame(upgma_dict.items(),columns=['cluster','ssdeep'])\n",
    "    upgma_df['c_ssdeep_li'] = upgma_df.ssdeep.map(create_hash_li)\n",
    "    upgma_df['score'] = upgma_df.apply(lambda x: compare(x.ssdeep, x.c_ssdeep_li), axis=1)\n",
    "    return upgma_df\n",
    "\n",
    "\n",
    "\n",
    "def draw_upgma(upgma_df,picture_dir=picture_dir,name='upgma'):\n",
    "    '''\n",
    "    GOAL: using upper triangle's distance to draw upgma\n",
    "    Input: from pair_wise_score() function\n",
    "    Output: diagram of UPGMA、Z info\n",
    "    '''\n",
    "    if not os.path.exists(picture_dir):\n",
    "        os.makedirs(picture_dir,exist_ok=True) \n",
    "    score_li = upgma_df['score'].tolist()\n",
    "    score_li = list(filter(None, score_li))\n",
    "    score_li = sum(score_li,[])\n",
    "    Z = linkage(score_li, 'average')\n",
    "    fig = plt.figure(figsize=(60, 24)) #(25,10) #(5,2)\n",
    "    # plt.savefig(fig)\n",
    "    dn = dendrogram(Z,labels=upgma_df.cluster.tolist())\n",
    "    plt.savefig(picture_dir+str(name)+'.png', dpi=600, format='png', bbox_inches='tight')\n",
    "    return dn,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knee_point(length_li,proto='http',date='20200110',k=100):\n",
    "    \"\"\"\n",
    "    每個list值所包含數量的變化量，也就是點的密集度變化量 (並非list自己本身值的變化量)\n",
    "    length_li: 長度的list\n",
    "    k: slope topK's knee points\n",
    "    Return dict format (key is the answer)\n",
    "    \"\"\"\n",
    "    length_dict = dict(Counter(length_li))\n",
    "    sorted_dict = {k: v for k, v in sorted(length_dict.items(), key=lambda x: x[1])}\n",
    "    all_items_num = sum(list(length_dict.values()))\n",
    "#     for item in sorted_dict.items():\n",
    "    all_keys = list(sorted_dict.keys())\n",
    "    all_values = list(sorted_dict.values())\n",
    "    slope_li = []\n",
    "    for i in range(len(sorted_dict)):\n",
    "        length1 = all_keys[i]\n",
    "        try:\n",
    "            length2 = all_keys[i+1]\n",
    "        except IndexError:\n",
    "            break\n",
    "        value1 = sum(all_values[:i+1])\n",
    "        value2 = sum(all_values[:i+2])\n",
    "        slope = ((value2-value1)/all_items_num)/(length2-length1)\n",
    "        slope_li.append(slope)\n",
    "    change_rate_li = []\n",
    "    for i in range(len(slope_li)):\n",
    "        try:\n",
    "            slope1 = slope_li[i]\n",
    "            slope2 = slope_li[i+1]\n",
    "        except IndexError:\n",
    "            break\n",
    "        change_rate_li.append(abs(slope2-slope1)) #陡變緩或是緩變陡的都一起算\n",
    "    idx_li = sorted(range(len(change_rate_li)), key=lambda i: change_rate_li[i], reverse=True)[:k]\n",
    "    return_dict = {}\n",
    "    for idx in idx_li:\n",
    "        return_dict[all_keys[idx+1]] = change_rate_li[idx]\n",
    "    ##畫圖可再自行修改\n",
    "    temp = sorted(length_li,reverse=True)\n",
    "    x_axis = [x for x in range(len(temp))] #隨便給個編號當成X軸\n",
    "    plt.figure(figsize=(30,20),dpi=300,linewidth = 1) # 圖片大小、折線寬度\n",
    "    plt.plot(x_axis,temp,'o-',color = 'b', label=proto) #折現的型態、折現的顏色\n",
    "#     plt.title(, x=0.5, y=1.03)\n",
    "    plt.figtext(.5,.9,str(date)+\" \"+str(proto)+\" clusters' size\",fontsize=30,ha='center')\n",
    "#     plt.xticks(fontsize=20)\n",
    "#     plt.yticks(fontsize=20)\n",
    "#     plt.xlabel(\"cluster ID\", fontsize=25, labelpad = 15)\n",
    "#     plt.ylabel(\"session #\", fontsize=25, labelpad = 20)\n",
    "    # plt.ylim(0, 2500)\n",
    "    # plt.legend(loc = \"best\", fontsize=20)\n",
    "    plt.savefig(picture_dir+str(proto)+'_'+str(date)+'_clustersize_line'+'.png', dpi=300, format='png')\n",
    "#     plt.show()\n",
    "    return pd.DataFrame.from_dict(return_dict,orient='index',columns=['knee_slope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_cluster(proto_big_dict,proto_loner_num,q1_num,proto,date):\n",
    "    '''\n",
    "    proto_big_dict: big dict from similarity_compare() func. (用以獲得各群的member數量)\n",
    "    proto_loner_num: the protocol's loner numbers\n",
    "    q1_num: 25%'s members# from\n",
    "    \n",
    "    Return: how many lesser is small cluster to deal as loner  (dict's member num)\n",
    "    '''\n",
    "    proto_dict_len = {}\n",
    "    for k,v in proto_big_dict.items():\n",
    "        proto_dict_len[k] = len(v)+1\n",
    "    temp = list(proto_dict_len.values())\n",
    "    for i in range(proto_loner_num):\n",
    "        temp.append(1)\n",
    "    temp = sorted(temp,reverse=True)\n",
    "    temp_df = knee_point(temp,proto=proto,date=date)\n",
    "    temp_score = temp_df.index.tolist()\n",
    "    try:\n",
    "        if temp_df.index[0] <q1_num:\n",
    "            return temp_df.index[0]-1\n",
    "        else:\n",
    "            for score in temp_score:\n",
    "                if score <q1_num:\n",
    "                    return int(score)-1 #往後找其他knee point要小於q1\n",
    "            return 0 #都不合<q1\n",
    "    except IndexError:\n",
    "        return 0 #群太少人太小分不出knee point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_cluster_info(protocol_loners,protocol_big_dict,protocol_df,pickle_dir,proto='http',df2=df2):\n",
    "    '''\n",
    "    GOAL: output loner & each cluster's three tuples pickles.\n",
    "    save file in pickle_dir+protol/ dir\n",
    "    \n",
    "    protocol_loners: from similarity_compare() func.\n",
    "    protocol_big_dict: from similarity_compare() func.\n",
    "    proto: now using what kind of protocol\n",
    "    df2: payload whole df\n",
    "    '''\n",
    "    loner_df = df2.loc[protocol_loners]\n",
    "    time_lists = loner_df.session_time_list.tolist()\n",
    "    time_lists = [list(x) for x in time_lists]\n",
    "    ips = loner_df.ip_src.tolist()\n",
    "    countries = loner_df.country.tolist()\n",
    "    pkl_dir = pickle_dir+proto+'/'\n",
    "    if not os.path.exists(pkl_dir):\n",
    "        os.makedirs(pkl_dir)\n",
    "    pickle.dump(obj=(time_lists,ips,countries),file=open(pkl_dir+'case#loners'+'_threetuples.pkl','wb'))\n",
    "    #cluster\n",
    "    protocol_big_dict = {k: v for k, v in sorted(protocol_big_dict.items(), key=lambda item: len(item[1]),reverse=True)}\n",
    "    for i,k in enumerate(protocol_big_dict.keys()):\n",
    "        write2pkl(protocol_df,protocol_big_dict,k,str(i+1)+\"_\"+str(k),path=pkl_dir)\n",
    "    return protocol_big_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run all protocols needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols_need = ['ssh','mysql','ftp','telnet','smb','http','pop','smtp','sip','imap','rpc']\n",
    "# protocols_need = ['ssh']\n",
    "# protocols_need = ['smb','rpc','imap','pop','smtp','mysql','sip','telnet','ftp','ssh','http'] #ori worked\n",
    "protocols_tuples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(proto,date,df2,pickle_dir,picture_dir): \n",
    "    '''\n",
    "    proto: protocol contains for e.g., http\n",
    "    date: day number e.g.,20200110\n",
    "    '''\n",
    "    proto_df, proto_df_payload = prepare_df(df2,proto)\n",
    "    proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict = similarity_compare(proto_df,proto_df_payload,thr=0)\n",
    "    print(\"==========\",str(proto)+\" clusters ==========\")\n",
    "    try:\n",
    "        proto_loner_num,stat_df = cluster_stat(proto_big_dict,proto_df,proto=proto,picture_dir=picture_dir,date=date)\n",
    "        proto_big_dict = case_cluster_info(proto_loners,proto_big_dict,proto_df,pickle_dir=pickle_dir,proto=proto,df2=df2)\n",
    "        small_clu_num = get_small_cluster(proto_big_dict,proto_loner_num,stat_df.loc['25%'].values[0],proto,date)\n",
    "#         print(\"===\",str(proto),\"loner threshold [small cluster / knee point]:\",small_clu_num+1,\"(include) ===\")\n",
    "        proto_upgma_dict = cluster_rep_hash(proto_big_dict,proto_df_payload,proto_df,proto_loners,knee_point=small_clu_num,date=date,protocol=proto)\n",
    "        pickle.dump(obj=proto_upgma_dict,file=open(pickle_dir+str(proto)+'_upgma_dict_'+str(date)+'.pkl','wb'))\n",
    "        gc.collect()\n",
    "        pickle.dump(obj=(proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                        proto_upgma_dict,stat_df),file=open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','wb'))\n",
    "    except KeyError:\n",
    "        print(\"!!!Didn't save:\",date,proto,\"!!!\")\n",
    "#     plt.clf()\n",
    "    plt.close()\n",
    "    gc.collect()\n",
    "#         return ()\n",
    "#     return (proto_df, proto_df_payload,proto_big_dict,proto_loners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mi'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Running date: mi 20200111 ===\n",
      "20200111 Done df2.\n",
      "==================== ftp clusters ==========\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群:  ssh clusters ==================== mysql clusters ==========\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群: 0 共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: \n",
      "20200111 mysql !!!\n",
      "========== telnet clusters ==========\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群: 0 共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 telnet !!!\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群: 0 共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 ssh !!!\n",
      "========== smb clusters ==========\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群: 0 共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 smb !!!\n",
      "========== pop clusters ==========\n",
      "原本總共sessions#: 00 共有#lonerSessions: 0\n",
      "\n",
      "共有#sessions可分群: 0 共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 pop !!!\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 ftp !!!\n",
      "========== sip clusters ==========\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群: 0 共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 sip !!!\n",
      "========== imap clusters ==========\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群: 0 共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 imap ==========!!!\n",
      " ========== rpc clusters ==========\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群: 0 ==========http clusters ==========共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 rpc !!!\n",
      "\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群: 0 共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 http !!!\n",
      " smtp clusters ==========\n",
      "原本總共sessions#: 0\n",
      "共有#sessions可分群: 0 共有#lonerSessions: 0\n",
      "可分為#群: 0\n",
      "!!!Didn't save: 20200111 smtp !!!\n",
      "CPU times: user 660 ms, sys: 43.5 ms, total: 703 ms\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#20hrs per day for all protocols\n",
    "\n",
    "date_li = ['20200101','20200102','20200103','20200104','20200105',\n",
    "           '20200113','20200114','20200115','20200116','20200117','20200118'] #改\n",
    "date_li = ['20200111'] #改\n",
    "# date_li = ['20200109','20200110','20200111','20200112']\n",
    "for da in date_li:\n",
    "    print(\"===Running date:\",isp,da,\"===\")\n",
    "    dir_path = \"/mnt/Raid160TB/pcap_inter/\"+str(da[:4])+'_'+str(da[4:6])+'_'+str(da[6:])+\"/\"+isp+\"/intermeidate_data/\"\n",
    "    saved_path = dir_path+\"df2_tuples_\"+isp+\".pkl\"\n",
    "    if path.exists(saved_path):\n",
    "        df2,pickle_dir,picture_dir = pickle.load(open(saved_path,'rb'))\n",
    "    else:\n",
    "        df2,pickle_dir,picture_dir = create_orifinalDF(da) #spark ram: 160G , df ram: 150G  \n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path,exist_ok=True)    \n",
    "        pickle.dump(file=open(dir_path+\"df2_tuples_\"+isp+\".pkl\",'wb'),protocol=pickle.HIGHEST_PROTOCOL,\n",
    "                obj=(df2,pickle_dir,picture_dir))\n",
    "    print(da,'Done df2.')\n",
    "    argli = []\n",
    "    for protocol in protocols_need:\n",
    "        argli.append((protocol,da,df2,pickle_dir,picture_dir))\n",
    "#     with Pool(processes=10) as pool:\n",
    "    gc.collect()\n",
    "    with ThreadPool(3) as pool:\n",
    "        results = pool.starmap(run_all,argli)\n",
    "#     print(results.get())\n",
    "    \n",
    "#     pool_result = []\n",
    "# #     pool = Pool(processes=2)\n",
    "#     pool = ThreadPool(4)\n",
    "#     for protocol in protocols_need:\n",
    "# #         r = pool.map_async(run_all,args=(protocol,da,df2,pickle_dir,picture_dir))\n",
    "#         r = pool.map_async(run_all,(protocol,da,df2,pickle_dir,picture_dir))\n",
    "#         pool_result.append(r)\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     for r in pool_result:\n",
    "#         print('return:',r.get())\n",
    "#     load_end = time.time() - load_start\n",
    "#     print(\"Process Time:\", '{:02f}:{:02f}:{:02f}'.format(load_end // 3600, (load_end % 3600 // 60), load_end % 60))\n",
    "    \n",
    "#     for protocol in tqdm(protocols_need):\n",
    "#         run_all(proto=protocol,date=da,df2=df2,pickle_dir=pickle_dir,picture_dir=picture_dir) ##改!!\n",
    "#         gc.collect()\n",
    "#     assert len(protocols_need) == len(protocols_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/Raid160TB/pcap_inter/2020_01_11/mi/intermeidate_data/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/mnt/Raid64TB2/intermediateOLD/\"+da+\"/\"\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path,exist_ok=True)    \n",
    "pickle.dump(file=open(dir_path+\"df2_tuples_\"+isp+\".pkl\",'wb'),protocol=pickle.HIGHEST_PROTOCOL,\n",
    "        obj=(df2,pickle_dir,picture_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df2['tcp_i_payload_list'].iloc[0]\n",
    "sorted(temp,key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合併三個dict用update來合併以後再通過後面的df function畫圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大家的pickle_dir都不同0109 0110 0111\n",
    "# time1 = '10'\n",
    "# time2 = '02'\n",
    "# time3 = '30'\n",
    "time_li = ['06','07','08','09','10','11','12']\n",
    "# time_li = ['02','10','30']\n",
    "proto = 'http'\n",
    "three_dict = {}\n",
    "for time in time_li:\n",
    "    path = '/home/antslab/spark_data/pcap_inter/2020_01_'+str(time)+'/中華電信/case_pickles/'+proto+'_upgma_dict_01'+str(time)+'.pkl'\n",
    "    temp = pickle.load(open(path,'rb'))\n",
    "    three_dict.update(temp)\n",
    "\n",
    "    \n",
    "loner_dict = {}\n",
    "for k,v in three_dict.items():\n",
    "    if '_L_' in k:\n",
    "        loner_dict[k]=v\n",
    "    if '_S_' in k:\n",
    "        loner_dict[k] =v\n",
    "\n",
    "loner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_upgma_df = pair_wise_score(three_dict) #改!!\n",
    "dn,Z = draw_upgma(three_upgma_df,\n",
    "           picture_dir='/home/antslab/spark_data/pcap_inter/2020_01_06/中華電信/case_pictures/' #改!!\n",
    "           ,name='0106-0112_upgma') #改!! 0106-0112_upgma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 畫跨天protocol大小圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_dict_time_all = {}\n",
    "trend_dict_country_all = {}\n",
    "trend_dict_ip_all = {}\n",
    "trend_dict_ssdeep_all = {}\n",
    "def draw_trend_pic(date,proto,pickle_dir,trend_dict_time_all=trend_dict_time_all,trend_dict_country_all=trend_dict_country_all,trend_dict_ip_all=trend_dict_ip_all,trend_dict_ssdeep_all=trend_dict_ssdeep_all):\n",
    "    '''\n",
    "    GOAL: output dict to draw trend-circle picture\n",
    "    \n",
    "    Return: 4 dict for the same proto for the next day\n",
    "    '''\n",
    "    try:\n",
    "        (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                proto_upgma_dict,stat_df) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "    except ValueError:\n",
    "        (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                proto_upgma_dict) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "    except FileNotFoundError:\n",
    "        return trend_dict_time_all,trend_dict_country_all,trend_dict_ip_all,trend_dict_ssdeep_all\n",
    "\n",
    "\n",
    "    def ssdeep_compare(target_hash,candidate_df):\n",
    "        '''\n",
    "        Input1: string of hash\n",
    "        Input2: dataframe of candidate\n",
    "        '''\n",
    "        def compare(candidate_hash):\n",
    "            if type(candidate_hash) == str:\n",
    "                return ssdeep.compare(target_hash,candidate_hash)\n",
    "            else:\n",
    "                score_li = []\n",
    "                for c_h in candidate_hash:\n",
    "                    score_li.append(ssdeep.compare(target_hash,c_h))\n",
    "                return max(score_li)\n",
    "        return candidate_df.hash.map(compare)\n",
    "\n",
    "    trend_dict_time = {}\n",
    "    trend_dict_country = {}\n",
    "    trend_dict_ip = {}\n",
    "    trend_dict_ssdeep = {}\n",
    "    candidate_df = pd.DataFrame(trend_dict_ssdeep_all.items(),columns=['idx','hash']) #其他已經有的cluster\n",
    "    for key,value in proto_big_dict.items():\n",
    "        target = proto_df_payload[proto_df_payload.idx == key]\n",
    "        target = target.reset_index(drop=True)\n",
    "        t_len = len(target)\n",
    "        q2 = int(t_len*0.5)\n",
    "        t_q2 = target.iloc[q2,-1] #要跟別人比較的cluster hash\n",
    "        if trend_dict_ssdeep_all == {}: #第一次\n",
    "            trend_dict_ssdeep[str(date)+\"_\"+str(key)] = t_q2\n",
    "            member_li = value[:]\n",
    "            member_li.append(key)\n",
    "            select_df = proto_df.loc[member_li]\n",
    "            time_li = select_df.session_time.tolist()\n",
    "            trend_dict_time[str(date)+\"_\"+str(key)] = time_li\n",
    "            country_li = select_df.country.tolist()\n",
    "            ip_li = select_df.ip_src.tolist()\n",
    "            assert len(time_li) == len(country_li) == len(ip_li)\n",
    "            trend_dict_country[str(date)+\"_\"+str(key)] = country_li\n",
    "            trend_dict_ip[str(date)+\"_\"+str(key)] = ip_li\n",
    "        else: #後面幾次\n",
    "            candidate_df['score'] = ssdeep_compare(t_q2,candidate_df) \n",
    "            max_score = candidate_df.score.max()\n",
    "            if max_score < 1: #都沒有相近的\n",
    "                trend_dict_ssdeep[str(date)+\"_\"+str(key)] = t_q2\n",
    "                member_li = value[:]\n",
    "                member_li.append(key)\n",
    "                select_df = proto_df.loc[member_li]\n",
    "                time_li = select_df.session_time.tolist()\n",
    "                trend_dict_time[str(date)+\"_\"+str(key)] = time_li\n",
    "                country_li = select_df.country.tolist()\n",
    "                ip_li = select_df.ip_src.tolist()\n",
    "                assert len(time_li) == len(country_li) == len(ip_li)\n",
    "                trend_dict_country[str(date)+\"_\"+str(key)] = country_li\n",
    "                trend_dict_ip[str(date)+\"_\"+str(key)] = ip_li\n",
    "            elif max_score>0: #有相近的合併到原本的dict\n",
    "                try:\n",
    "                    combine_id = candidate_df[candidate_df.score == max_score].idx.tolist()[0]\n",
    "                except:\n",
    "                    print( candidate_df,max_score,target)\n",
    "                ori_ssdeep_list = trend_dict_ssdeep_all[combine_id][:]\n",
    "                if type(ori_ssdeep_list) == str:\n",
    "                    ori_ssdeep_list = [ori_ssdeep_list] #之前只有一個ssdeep hash\n",
    "                ori_time_list = trend_dict_time_all[combine_id][:]\n",
    "                ori_country_list = trend_dict_country_all[combine_id][:]\n",
    "                ori_ip_list = trend_dict_ip_all[combine_id][:]\n",
    "                member_li = value[:]\n",
    "                member_li.append(key)\n",
    "                select_df = proto_df.loc[member_li]\n",
    "                time_li = select_df.session_time.tolist()\n",
    "                ori_time_list.extend(time_li)\n",
    "                country_li = select_df.country.tolist()\n",
    "                ip_li = select_df.ip_src.tolist()\n",
    "                ori_country_list.extend(country_li)\n",
    "                ori_ip_list.extend(ip_li)\n",
    "                ori_ssdeep_list.append(t_q2) #我合併進入別人的群，所以把我群的key hash也加入\n",
    "                assert len(ori_time_list) == len(ori_country_list) == len(ori_ip_list)\n",
    "                trend_dict_time_all[combine_id] = ori_time_list\n",
    "                trend_dict_country_all[combine_id] = ori_country_list\n",
    "                trend_dict_ip_all[combine_id] = ori_ip_list\n",
    "                trend_dict_ssdeep_all[combine_id] = ori_ssdeep_list\n",
    "            else:\n",
    "                print(max_score)\n",
    "#         break #debug\n",
    "    trend_dict_time_all.update(trend_dict_time)\n",
    "    trend_dict_country_all.update(trend_dict_country)\n",
    "    trend_dict_ip_all.update(trend_dict_ip)\n",
    "    trend_dict_ssdeep_all.update(trend_dict_ssdeep)\n",
    "    return trend_dict_time_all,trend_dict_country_all,trend_dict_ip_all,trend_dict_ssdeep_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_li(time_li, country_li):\n",
    "    '''\n",
    "    GOAL: sort by time (align with time's order)\n",
    "    Return: list\n",
    "    '''\n",
    "    sort_country_li = [x for _,x in sorted(zip(time_li,country_li))]\n",
    "    return sort_country_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# date_li = [\"0110\",\"0130\",\"0102\"]\n",
    "date_li = [\"0102\",\"0106\",\"0107\",\"0108\",\"0109\",\"0110\",\"0111\",\"0112\",\"0130\"] #,\"0102\",\"0130\"\n",
    "proto_li = ['tds','http','smb','telnet','ftp','smtp','mysql','ssh','rpc','imap','pop','sip']\n",
    "for proto in tqdm(proto_li):\n",
    "    trend_dict_time_all = {}\n",
    "    trend_dict_country_all = {}\n",
    "    trend_dict_ip_all = {}\n",
    "    trend_dict_ssdeep_all = {}\n",
    "    for date in date_li:\n",
    "        pickle_dir = '/home/antslab/spark_data/pcap_inter/2020_01_'+str(date[-2:])+'/中華電信/case_pickles/'\n",
    "        trend_dict_time_all,trend_dict_country_all,trend_dict_ip_all,trend_dict_ssdeep_all = draw_trend_pic(date,proto,\n",
    "                                                                                          pickle_dir,trend_dict_time_all,trend_dict_country_all,trend_dict_ip_all,trend_dict_ssdeep_all)\n",
    "    trend_dict_time_all = {k: v for k, v in sorted(trend_dict_time_all.items(), key=lambda item: len(item[1]),reverse=True)}\n",
    "    time_df = pd.DataFrame(trend_dict_time_all.items(),columns=['idx','timestamp'])\n",
    "    trend_dict_country_all = {k: v for k, v in sorted(trend_dict_country_all.items(), key=lambda item: len(item[1]),reverse=True)}\n",
    "    country_df = pd.DataFrame(trend_dict_country_all.items(),columns=['idx','country'])\n",
    "    trend_dict_ip_all = {k: v for k, v in sorted(trend_dict_ip_all.items(), key=lambda item: len(item[1]),reverse=True)}\n",
    "    ip_df =  pd.DataFrame(trend_dict_ip_all.items(),columns=['idx','src_ip'])\n",
    "    ssdeep_df = pd.DataFrame(trend_dict_ssdeep_all.items(),columns=['idx','ssdeep'])\n",
    "    all_df = pd.merge(time_df,country_df,on='idx')\n",
    "    all_df =  pd.merge(all_df,ip_df,on='idx')\n",
    "    all_df = pd.merge(all_df,ssdeep_df,on='idx')\n",
    "    all_df['country'] = all_df.apply(lambda x: sort_li(x.timestamp, x.country), axis=1)\n",
    "    all_df['src_ip'] = all_df.apply(lambda x: sort_li(x.timestamp, x.src_ip), axis=1)\n",
    "    all_df['timestamp'] = all_df.timestamp.map(sorted)\n",
    "    file_name = \"_\".join(sorted(date_li))\n",
    "    pickle.dump(obj=all_df,\n",
    "                file=open('/home/antslab/spark_data/pcap_inter/2020_01_'+str(min(date_li)[-2:])+'/中華電信/case_pickles/'+proto+'_trend_df_'+file_name+'.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proto = 'http'\n",
    "date_li = [\"0102\",\"0106\",\"0107\",\"0108\",\"0109\",\"0110\",\"0111\",\"0112\",\"0130\"] #,\"0102\",\"0130\"\n",
    "file_name = \"_\".join(sorted(date_li))\n",
    "\n",
    "http_df = pickle.load(open('/home/antslab/spark_data/pcap_inter/2020_01_'+str(min(date_li)[-2:])+'/中華電信/case_pickles/'+proto+'_trend_df_'+file_name+'.pkl','rb'))\n",
    "http_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#輸出cluster之key session的time list\n",
    "wireshark_li = http_df.idx.tolist() #.head(15)前15大cluster #改!proto\n",
    "wireshark_rank = []\n",
    "for i,v in enumerate(wireshark_li):\n",
    "    wireshark_rank.append(i+1)\n",
    "wireshark_rank = [x for _,x in sorted(zip(wireshark_li,wireshark_rank))]\n",
    "wireshark_li = sorted(wireshark_li)\n",
    "\n",
    "save_path_li = []\n",
    "now_date = '00' #現在正在處理的日期\n",
    "for i,wireshark in tqdm(zip(wireshark_rank,wireshark_li)):\n",
    "    date = wireshark.split('_')[0] #該cluster key的同月份日期\n",
    "    if date!= now_date: #新日期才要重讀\n",
    "        now_date = date\n",
    "        \n",
    "        pickle_dir = '/home/antslab/spark_data/pcap_inter/2020_01_'+str(date[-2:])+'/中華電信/case_pickles/'\n",
    "        try:\n",
    "            (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                    proto_upgma_dict,stat_df) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "        except ValueError:\n",
    "            (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                    proto_upgma_dict) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    idx = wireshark.split('_')[-1] #該cluster在該日期該proto的df中的index\n",
    "    #     else:\n",
    "    try:\n",
    "        time_list = proto_df.loc[int(idx),'session_time_list'].tolist()\n",
    "    except AttributeError:\n",
    "        time_list = proto_df.loc[int(idx),'session_time_list']\n",
    "    timelist_path = pickle_dir+'timelist_'+proto+'_large#'+str(i)+'_clusterID#'+str(idx)+'.pkl'\n",
    "    pickle.dump(file=open(timelist_path,'wb'),obj=time_list)\n",
    "    print(wireshark,'save in:',timelist_path)\n",
    "    save_path_li.append(timelist_path)\n",
    "\n",
    "first_date = wireshark_li[0].split('_')[0][-2:]\n",
    "save_path = '/home/antslab/spark_data/pcap_inter/2020_01_'+first_date+'/中華電信/case_pickles/'+proto+'_clusterKey_timelist_paths.pkl'\n",
    "pickle.dump(file=open(save_path,'wb'),obj=save_path_li)  \n",
    "print('all paths in:',save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distinct_number(li):\n",
    "    return len(set(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_df['ip_num'] = http_df.src_ip.map(count_distinct_number)\n",
    "http_df['ip_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小群關聯\n",
    "* L、S\n",
    "* src_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loner_protocol_dict = {}\n",
    "loner_time_dict = {}\n",
    "date_li = [\"0110\",\"0111\",\"0109\",\"0112\",\"0108\",\"0107\",\"0106\"] #,\"0102\",\"0130\"\n",
    "proto_li = ['http','smb','telnet','ftp','smtp','mysql','ssh','rpc','imap','pop','sip']\n",
    "# for proto in tqdm(proto_li):\n",
    "for date in date_li:\n",
    "    pickle_dir = '/home/antslab/spark_data/pcap_inter/2020_01_'+str(date[-2:])+'/中華電信/case_pickles/'\n",
    "    try:\n",
    "        (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                proto_upgma_dict,stat_df) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "    except ValueError:\n",
    "        (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                proto_upgma_dict) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    for k,v in proto_upgma_dict.items():\n",
    "        if '_L_' not in k: #只拿Loner來做關聯\n",
    "            continue\n",
    "        lonerInfo = k.split('_')\n",
    "        loner_ip = lonerInfo[-1]\n",
    "        loner_counry = lonerInfo[-3]\n",
    "        loner_domain = lonerInfo[-2]\n",
    "        for d in date_li:\n",
    "            in_file = '/home/antslab/data_hdd4t/pcap_process/pcap_spark/2020_01_'+str(d[-2:])+'/中華電信/parquet/2020_01_'+str(date[-2:])+'_session_v7-2_hashpayload_time_size.parquet' #改!!\n",
    "            df_loner = ks.read_parquet(in_file)\n",
    "            ip_df = df_loner[df_loner.ip_src == loner_ip]\n",
    "            ip_df = ip_df.to_pandas()\n",
    "            try:\n",
    "                ori_loner_time_li = loner_time_dict['L_'+loner_counry+'_'+loner_domain+'_'+loner_ip][:]\n",
    "                ori_loner_protocol_li = loner_protocol_dict['L_'+loner_counry+'_'+loner_domain+'_'+loner_ip][:]\n",
    "                ori_loner_time_li.extend(ip_df.session_time.tolist())\n",
    "                ori_loner_protocol_li.extend(ip_df.frame_i_max_protocols.tolist())\n",
    "                loner_time_dict['L_'+loner_counry+'_'+loner_domain+'_'+loner_ip] = ori_loner_time_li\n",
    "                loner_protocol_dict['L_'+loner_counry+'_'+loner_domain+'_'+loner_ip] = ori_loner_protocol_li\n",
    "            except KeyError:\n",
    "                loner_time_dict['L_'+loner_counry+'_'+loner_domain+'_'+loner_ip] = ip_df.session_time.tolist()\n",
    "                loner_protocol_dict['L_'+loner_counry+'_'+loner_domain+'_'+loner_ip] = ip_df.frame_i_max_protocols.tolist()\n",
    "    break #debug\n",
    "#     break #debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_upgma_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = '/home/antslab/data_hdd4t/pcap_process/pcap_spark/2020_01_10/中華電信/parquet/2020_01_10_session_v7-2_hashpayload_time_size.parquet'\n",
    "df = ks.read_parquet(in_file)\n",
    "ip_df = df[df.ip_src == '200.255.122.170']\n",
    "ip_df = ip_df.to_pandas()\n",
    "gc.collect()\n",
    "ip_df['frame_i_max_protocols'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畫distribution\n",
    "* 針對某單一case進行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def case_study(idx,payload_df):\n",
    "    '''\n",
    "    idx: which index is the dataframe's target?\n",
    "    payload_df: protocol payload dataframe (packet-based)\n",
    "    下面有一些參數可以改 (line19~23、27註解處)\n",
    "    \n",
    "    Return: Big dict\n",
    "    '''\n",
    "    big_dict = {}\n",
    "    idx = int(idx)\n",
    "    target = payload_df[payload_df.idx == idx]\n",
    "    target = target.reset_index(drop=True)\n",
    "    t_len = len(target)\n",
    "    t_q1 = target.iloc[int(t_len*0.25),-1]\n",
    "    t_q2 = target.iloc[int(t_len*0.5),-1]\n",
    "    t_q3 = target.iloc[int(t_len*0.75),-1]\n",
    "    candidate = payload_df[payload_df.idx != idx]\n",
    "    candidate['idx'] = candidate.idx.astype(str)\n",
    "    candidate_q1 = candidate[candidate['size']==target.iloc[int(t_len*0.25),-2]] #是否要match: match改成== 不match改成!=\n",
    "    candidate['q1'] = ssdeep_compare(t_q1,candidate_q1)\n",
    "    candidate_q2 = candidate[candidate['size']==target.iloc[int(t_len*0.5),-2]] # == or != (score=0)\n",
    "    candidate['q2'] = ssdeep_compare(t_q2,candidate_q2)\n",
    "    candidate_q3 = candidate[candidate['size']==target.iloc[int(t_len*0.75),-2]] #要找不像的時候要改成!=\n",
    "    candidate['q3'] = ssdeep_compare(t_q3,candidate_q3)\n",
    "    candidate = candidate.drop(['size','hash'],axis=1)\n",
    "    candidate['score'] = candidate.max(axis=1)\n",
    "    candidate = candidate[candidate.score>0] #相似度分數，數字越小速度愈快、數字越大越多群。要抓出幾分的人> == <\n",
    "    candidate['idx'] = candidate.idx.astype(int)\n",
    "    idx_li = candidate.idx.tolist()\n",
    "    if len(idx_li)>0:\n",
    "        big_dict[idx] = idx_li\n",
    "    # len(big_dict.keys())\n",
    "    print(\"所有該protocol與所給定之target的分數種類:\",candidate.score.unique())\n",
    "    print(\"總session數量:\",len(big_dict[idx]))\n",
    "    print(\"分數的分布:\",candidate.score.value_counts().sort_index())\n",
    "    print(\"分數的分布圖:\",candidate.score.hist(bins=25))\n",
    "    return big_dict, candidate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
