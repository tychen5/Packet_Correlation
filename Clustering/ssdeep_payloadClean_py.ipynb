{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssdeep\n",
    "import pickle,os,sys,gc\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "import subprocess\n",
    "from multiprocessing import Pool \n",
    "import time,datetime\n",
    "from itertools import chain\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import functools\n",
    "import itertools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antslab/anaconda3/bin/python\n",
      "3.7.7 (default, Mar 26 2020, 15:48:22) \n",
      "[GCC 7.3.0]\n",
      "sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\n",
      "1.2.0 3.0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mike-Z370:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[12]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Leo_spark_123</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5c2bf7c7d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "import findspark\n",
    "# os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3' #'/home/antslab/anaconda3/bin/python'\n",
    "findspark.init()\n",
    "import pyspark\n",
    "# from pyspark import SparkContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row, SQLContext, SparkSession, window\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "import  pyspark.sql.functions as F\n",
    "conf = SparkConf()\n",
    "# conf.set(\"spark.local.dir\", \"/mnt/ssd240g/data/Leo_Spark_Home/tmp\")\n",
    "conf.set(\"spark.executor.cores\",\"12\")\n",
    "conf.setMaster(\"local[12]\")\n",
    "conf.set(\"spark.driver.memory\",\"255g\") #滿載165G/\n",
    "conf.set(\"spark.executor.memory\", \"100g\")\n",
    "conf.set(\"spark.memory.offHeap.enabled\",\"true\")\n",
    "conf.set(\"spark.memory.offHeap.size\",\"95g\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\",\"-Xss85g\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\",\"-Xms85g\")\n",
    "# conf.set(\"spark.memory.storageFraction\",\"0.2\")\n",
    "# conf.set(\"spark.driver.maxResultSize\",\"0\")\n",
    "# conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "# conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "conf.set(\"spark.kubernetes.pyspark.pythonVersion\",\"3\")\n",
    "conf.set(\"spark.network.timeout\",\"1500s\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\",130000)\n",
    "conf.set(\"spark.driver.maxResultSize\", \"160g\")\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"pcap_analyzer\").config(conf=conf).getOrCreate() #pcap_analyzer pcapAnalyzer\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Leo_spark_123\").config(conf=conf).getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 300000)\n",
    "spark.conf.set(\"spark.sql.hive.filesourcePartitionFileCacheSize\",110*1024*1024*1024)\n",
    "\n",
    "import databricks.koalas as ks\n",
    "ks.options.display.max_rows = 20\n",
    "ks.set_option('compute.max_rows', None)\n",
    "# ks.set_option('compute.ops_on_diff_frames', True)\n",
    "ks.set_option('compute.default_index_type', 'distributed')\n",
    "print(ks.__version__,spark.version)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "isp = '台灣大哥大' #指定ISP目錄\n",
    "protocols_need = ['ssh','mysql','ftp','telnet','smb','http','pop','smtp','sip','imap','rpc'] #指定protocol\n",
    "date_li = ['20200106','20200107','20200108','20200109','20200110','20200111','20200112'] #指定日期\n",
    "\n",
    "\n",
    "in_file = np.nan\n",
    "pickle_dir = np.nan\n",
    "picture_dir = np.nan\n",
    "df2 = \"\"\n",
    "def create_orifinalDF(time):\n",
    "    \"\"\"\n",
    "    GOAL: preprocess original df\n",
    "    time: e.g., 20200101\n",
    "    \n",
    "    Return: df2\n",
    "    \"\"\"\n",
    "    \n",
    "    global in_file,pickle_dir,picture_dir #in file HDFS?\n",
    "    in_file = 'hdfs://192.168.50.123:8020/user/hdfs/parquet/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/'+isp+'/session_parquet/'+str(time)+'_session.parquet'\n",
    "    pickle_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/'+isp+'/case_pickles/'\n",
    "    picture_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/'+isp+'/case_pictures/'\n",
    "    \n",
    "    if not os.path.exists(pickle_dir):\n",
    "        os.makedirs(pickle_dir,exist_ok=True)\n",
    "    if not os.path.exists(picture_dir):\n",
    "        os.makedirs(picture_dir,exist_ok=True)    \n",
    "    df = ks.read_parquet(in_file)\n",
    "    df = df[['session_time','session_time_list','session_duration', 'session_i_tt_packet', 'session_o_tt_packet',\n",
    "                 'session_i_tt_frame_length','session_o_tt_frame_length', 'udp_i_tt_length',\n",
    "                 'udp_o_tt_length', 'udp_i_avg_length', 'udp_o_avg_length','icmp_i_avg_length', 'icmp_o_avg_length',\n",
    "                 'icmp_i_tt_datagram_length','icmp_o_tt_datagram_length',\n",
    "                 'tcp_i_tt_payload_length','tcp_o_tt_payload_length', 'tcp_i_avg_payload_length',\n",
    "                 'tcp_o_avg_payload_length','ip_src', 'ip_dst','tcp_srcport', 'tcp_dstport', 'country','isp',\n",
    "                 'domain', 'frame_i_max_protocols','frame_o_max_protocols', 'tcp_i_payload_list', 'tcp_o_payload_list'        \n",
    "                ]]\n",
    "     #篩選出有in bound payload的session\n",
    "    df1 = df[(df.session_duration>0.1)&(df.session_i_tt_packet>1)&(df.session_i_tt_frame_length>0)&\n",
    "             (df.tcp_i_payload_list.astype(str)!='[]')]\n",
    "    df1 = df1[(df1.udp_i_tt_length>0)|(df1.udp_i_avg_length>0)|(df1.icmp_i_avg_length>0)|\n",
    "            (df1.icmp_i_tt_datagram_length>0)|(df1.tcp_i_tt_payload_length>0)|(df1.tcp_i_avg_payload_length>0)]\n",
    "\n",
    "    df2 = df1[(df1.tcp_o_payload_list.astype(str)!='[]')&(df1.session_o_tt_packet>0)&(df1.session_o_tt_frame_length>0)]\n",
    "    df2 = df2[(df2.udp_o_tt_length>0)|(df2.udp_o_avg_length>0)|(df2.icmp_o_avg_length>0)|\n",
    "            (df2.icmp_o_tt_datagram_length>0)|(df2.tcp_o_tt_payload_length>0)|(df2.tcp_o_avg_payload_length>0)]\n",
    "    df2 = df2[(df2.domain != 'googlebot.com')|(df2.isp!='Googlebot')] #\n",
    "    gc.collect()\n",
    "    df2 = df2.to_pandas() #記憶體可能會不夠?\n",
    "    return df2,pickle_dir,picture_dir#,df2_tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df2,protocol_name='http',split=\"size\",loner=False):\n",
    "    protocol_name = str(protocol_name).lower()\n",
    "    '''\n",
    "    df2: filterout no payload's all sessions df\n",
    "    protocol_name: what TCP protocol string would be contained e.g., smb,http,tds...\n",
    "    loner: 用來檢查不同天相同protocol的分群\n",
    "    \n",
    "    return1: protocol original dataframe (session-based)\n",
    "    return2: protocol payload dataframe (packet-based)\n",
    "    '''\n",
    "    def sort_fn(data):\n",
    "        '''\n",
    "        sort by time: itemgetter(1)\n",
    "        sort by size: itemgetter(2)\n",
    "        '''\n",
    "        if split=='size':\n",
    "            return sorted(data,key=itemgetter(2))\n",
    "        elif split == 'time':\n",
    "            return sorted(data,key=itemgetter(1))\n",
    "                    \n",
    "    def split_col_len(session):\n",
    "        '''\n",
    "        input: list(session) of lists(packets) =>tuple=(hash,time)\n",
    "\n",
    "        Return1: list of ssdeep length\n",
    "        Return2: list of session's packets hashes\n",
    "        '''\n",
    "    #     for session in payload_li:\n",
    "#         packet_payload = []\n",
    "        packet_len = []\n",
    "        for packet in session:\n",
    "            ssdeep_hash = packet[0]\n",
    "    #         packet_payload.append(ssdeep_hash)\n",
    "            packet_len.append(ssdeep_hash.split(':')[0])\n",
    "        return packet_len#,packet_payload\n",
    "\n",
    "    def split_col_hash(session):\n",
    "        '''\n",
    "        input: list(session) of lists(packets) =>tuple=(hash,time)\n",
    "\n",
    "        Return1: list of ssdeep length\n",
    "        Return2: list of session's packets hashes\n",
    "        '''\n",
    "        packet_payload = []\n",
    "        for packet in session:\n",
    "            ssdeep_hash = packet[0]\n",
    "            packet_payload.append(ssdeep_hash)\n",
    "        return packet_payload\n",
    "    if not loner:\n",
    "        df2_protocol = df2[(df2.frame_i_max_protocols.str.contains(protocol_name))&(df2.frame_o_max_protocols.str.contains(protocol_name))]\n",
    "    else:\n",
    "        df2_protocol = df2\n",
    "    df2_protocol_payload = df2_protocol[['tcp_i_payload_list']]\n",
    "    df2_protocol_payload['tcp_i_payload_list'] = df2_protocol_payload.tcp_i_payload_list.apply(sort_fn) #map\n",
    "    df2_protocol_payload['size'] = df2_protocol_payload.tcp_i_payload_list.map(split_col_len) #,df2_protocol_payload['hash']\n",
    "    df2_protocol_payload['hash'] = df2_protocol_payload.tcp_i_payload_list.map(split_col_hash)\n",
    "    L_size = [x if isinstance(x, list) else [x] for x in df2_protocol_payload['size']]\n",
    "    L_hash = [x if isinstance(x, list) else [x] for x in df2_protocol_payload['hash']]\n",
    "    df2_protocol_payload = pd.DataFrame({\n",
    "        'idx':df2_protocol_payload.index.values.repeat([len(x) for x in L_size]),\n",
    "        'size':list(chain.from_iterable(L_size)),\n",
    "        'hash':list(chain.from_iterable(L_hash))\n",
    "        })\n",
    "    df2_protocol_payload['size'] = df2_protocol_payload['size'].astype(int)\n",
    "    if not loner:\n",
    "        return df2_protocol, df2_protocol_payload\n",
    "    else:\n",
    "        return df2_protocol_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_compare(df2_protocol,df2_protocol_payload,ori_protocol=None,ori_protocol_payload=None,thr=0,thr2=10,loner=False): #40\n",
    "    '''\n",
    "    GOAL: compare with timestamp's q1,q2,q3 to similar size packet. Pick max value for score.\n",
    "    df2_protocol: protocol original dataframe (session-based)\n",
    "    df2_protocol_payload: protocol payload dataframe (packet-based)\n",
    "    loner:需指派ori_protocol、ori_protocol_payload\n",
    "    \n",
    "    Return: dictionary with each cluster \n",
    "    '''\n",
    "    def ssdeep_compare(target_hash,candidate_df):\n",
    "        '''\n",
    "        Input1: string of hash\n",
    "        Input2: dataframe of candidate\n",
    "        '''\n",
    "        def compare(candidate_hash):\n",
    "            return ssdeep.compare(target_hash,candidate_hash)\n",
    "        return candidate_df.hash.map(compare)\n",
    "    big_dict_protocol = {}\n",
    "    big_dict_protocol_score = {}\n",
    "    used_idx_li = []\n",
    "    all_scores = [] #test2\n",
    "    for idx in df2_protocol.index:\n",
    "        if idx in used_idx_li:\n",
    "            continue\n",
    "        target = df2_protocol_payload[df2_protocol_payload.idx == idx]\n",
    "        target = target.reset_index(drop=True)\n",
    "        t_len = len(target)\n",
    "        q1 = int(t_len*0.25)\n",
    "        q2 = int(t_len*0.5)\n",
    "        t_q1 = target.iloc[q1,-1]\n",
    "        if not loner:\n",
    "            candidate = df2_protocol_payload[df2_protocol_payload.idx!=idx]\n",
    "            candidate = candidate[~candidate.idx.isin(used_idx_li)] #candidat不可重複分群\n",
    "        else:\n",
    "            t_ip = df2_protocol[df2_protocol.index == idx].ip_src.values[0]\n",
    "            candidate_idx = ori_protocol[ori_protocol.ip_src == t_ip].index.tolist()\n",
    "            candidate = ori_protocol_payload[ori_protocol_payload.idx.isin(candidate_idx)] #可重複分群?\n",
    "        \n",
    "        candidate['idx'] = candidate.idx.astype(str)\n",
    "        candidate['q1'] = ssdeep_compare(t_q1,candidate) #rule:candidate_q1\n",
    "        if q2 != q1:\n",
    "            t_q2 = target.iloc[q2,-1]\n",
    "            q3 = int(t_len*0.75)\n",
    "            candidate['q2'] = ssdeep_compare(t_q2,candidate) #rule:candidate_q2\n",
    "            if q3 != q2:\n",
    "                t_q3 = target.iloc[q3,-1]\n",
    "                candidate['q3'] = ssdeep_compare(t_q3,candidate) #rule:candidate_q3\n",
    "                if t_len>4:\n",
    "                    t_max = target.iloc[-1,-1]\n",
    "                    candidate['max'] = ssdeep_compare(t_max,candidate)\n",
    "                    \n",
    "        candidate = candidate.drop(['size','hash'],axis=1)\n",
    "        candidate['score'] = candidate.mean(axis=1) #max\n",
    "        score_li = candidate.score.tolist() #test2\n",
    "        candidate = candidate[candidate.score>thr] #相似度分數，數字越小速度愈快、數字越大越多群\n",
    "        candidate['idx'] = candidate.idx.astype(int)\n",
    "        idx_li = list(set(candidate.idx.tolist()))\n",
    "        all_scores.extend(score_li) #test2\n",
    "        if len(idx_li)>0:\n",
    "            \n",
    "            used_idx_li.extend(idx_li)\n",
    "            used_idx_li.append(idx)\n",
    "            if not loner:\n",
    "                big_dict_protocol[idx] = idx_li\n",
    "                big_dict_protocol_score[idx] = candidate.score.tolist()\n",
    "            else:\n",
    "                big_dict_protocol[t_ip+'_'+str(idx)] = idx_li\n",
    "                big_dict_protocol_score[t_ip+'_'+str(idx)] = candidate.score.tolist()                \n",
    "    return big_dict_protocol,sorted(list(set(df2_protocol.index)-set(used_idx_li))),all_scores,big_dict_protocol_score #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write2pkl(df2_protocol,big_dict,cluster_key,case_id,path=pickle_dir):\n",
    "    '''\n",
    "    GOAL: output 3-tuple to draw time diagram\n",
    "    df2_protocol: original dataframe session-based\n",
    "    big_dict: big_dict_protocol\n",
    "    cluster_key: print by find_cluster function\n",
    "    case_id: user defined to identify\n",
    "    '''\n",
    "    cluster_key = int(cluster_key)\n",
    "    temp_li = big_dict[cluster_key][:]\n",
    "    temp_li.append(cluster_key) #最後一個session才是key (target)，其他人(candidates)都是跟他(target)比\n",
    "    case_diagram = df2_protocol.loc[list(set(temp_li))]\n",
    "    time_lists = case_diagram.session_time_list.tolist()\n",
    "    time_lists = [list(x) for x in time_lists]\n",
    "    ips = case_diagram.ip_src.tolist()\n",
    "    countries = case_diagram.country.tolist()\n",
    "    pickle.dump(obj=(time_lists,ips,countries),file=open(path+'case#'+str(case_id)+'_threetuples.pkl','wb'))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster(big_dict,idx):\n",
    "    '''\n",
    "    big_dict: big_dict_protocol\n",
    "    idx: which idx want to find\n",
    "    Return cluster id (big_dict\"s key number')\n",
    "    '''\n",
    "    idx = int(idx)\n",
    "    try:\n",
    "        big_dict[idx] #KeyError\n",
    "        return idx\n",
    "    except KeyError:\n",
    "        for k,v in big_dict.items():\n",
    "            if idx in v:\n",
    "                return k\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_stat(protocol_big_dict,protocol_df,proto,date,picture_dir,drawlog=False):\n",
    "    \"\"\"\n",
    "    GOAL: output statistic of certain protocol\n",
    "    input1: dict from similarity_compare\n",
    "    inpit2: df from prepare_df\n",
    "    \"\"\"\n",
    "    \n",
    "    all_nums = []\n",
    "    clus_id = []\n",
    "    # all_num = 0\n",
    "    for k,v in protocol_big_dict.items():\n",
    "    #     all_nums.extend(v)\n",
    "        all_nums.append(len(v))\n",
    "        clus_id.append(k)\n",
    "    #     all_num +=len(v)\n",
    "    clus_num = len(all_nums)\n",
    "    print(\"原本總共sessions#:\",len(protocol_df))\n",
    "    print(\"共有#sessions可分群:\",sum(all_nums)+clus_num,\"共有#lonerSessions:\",len(protocol_df)-(sum(all_nums)+clus_num))\n",
    "    print(\"可分為#群:\",clus_num)\n",
    "#     print(sum(all_nums)+len(all_nums),len(protocol_df)-(sum(all_nums)+len(all_nums)),len(protocol_df))\n",
    "    protocol_stat_df = pd.DataFrame(all_nums,index=clus_id)\n",
    "    protocol_stat_df[0] = protocol_stat_df[0]+1\n",
    "    print(\"前五大的cluster key與對應之群集大小\\n\",protocol_stat_df[0].nlargest(5))\n",
    "    print(protocol_stat_df.describe())\n",
    "    protocol_stat_df  = pd.DataFrame(all_nums,index=[x for x in range(len(all_nums))])\n",
    "    protocol_stat_df.hist(bins=100)\n",
    "    plt.savefig(picture_dir+str(proto)+'_'+str(date)+'_clustersize_hist'+'.png', dpi=300, format='png')\n",
    "    \n",
    "    temp = dict(Counter(all_nums))\n",
    "    temp = {k: v for k, v in sorted(temp.items(), key=lambda item: item[0])}\n",
    "#     temp = sorted(all_nums,reverse=True)\n",
    "    x_axis = [k for k in temp.keys()] #群中所含的數量\n",
    "    y_axis = [v for v in temp.values()] #該數量共有幾群為此\n",
    "    plt.figure(figsize=(30,20),dpi=300,linewidth = 1) # 圖片大小、折線寬度\n",
    "    plt.plot(x_axis,y_axis,'o-',color = 'b', label=proto) #折現的型態、折現的顏色\n",
    "#     plt.title(, x=0.5, y=1.03)\n",
    "    plt.figtext(.5,.9,str(date)+\" \"+str(proto)+\" cluster# of cluster's size\",fontsize=30,ha='center')\n",
    "#     plt.xticks(fontsize=20)\n",
    "#     plt.yticks(fontsize=20)\n",
    "    plt.xlabel(\"cluster size\", fontsize=25, labelpad = 15)\n",
    "    plt.ylabel(\"cluster #\", fontsize=25, labelpad = 20)\n",
    "    # plt.ylim(0, 2500)\n",
    "    # plt.legend(loc = \"best\", fontsize=20)\n",
    "    plt.savefig(picture_dir+str(proto)+'_'+str(date)+'_clusternumber_line'+'.png', dpi=300, format='png')\n",
    "#     plt.show()    \n",
    "    \n",
    "#log diagram\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_yscale('log')\n",
    "    protocol_stat_df.hist(ax=ax, bins=100, bottom=0.1)\n",
    "    plt.savefig(picture_dir+str(proto)+'_'+str(date)+'_clustersize_hist_log'+'.png', dpi=300, format='png')\n",
    "    \n",
    "    return len(protocol_df)-(sum(all_nums)+clus_num),protocol_stat_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_rep_hash(proto_big_dict,proto_df_payload,proto_df,proto_loners,knee_point,date='0110',protocol='http'):\n",
    "    '''\n",
    "    GOAL: create each cluster's representaion ssdeep hash\n",
    "    proto_df_payload: from prepare_df() function\n",
    "    proto_big_dict: from similarity_compare() function\n",
    "    proto_loners: loner idx list\n",
    "    knee_point: from get_small_cluster() function\n",
    "    date & protocol: user_defined cluster name\n",
    "    \n",
    "    Return: dict[cluster_name]: ssdeep hash\n",
    "    '''\n",
    "    upgma_dict = {}\n",
    "    for key,value in proto_big_dict.items(): #cluster rep ssdeep\n",
    "        target = proto_df_payload[proto_df_payload.idx == key]\n",
    "        target = target.reset_index(drop=True)\n",
    "        t_len = len(target)\n",
    "        q2 = int(t_len*0.5)\n",
    "        q2_hash = target.iloc[q2,-1]\n",
    "        country = proto_df.loc[key,'country']\n",
    "        ip = proto_df.loc[key,'ip_src']\n",
    "        domain = proto_df.loc[key,'domain']\n",
    "        if len(value)>knee_point:\n",
    "            upgma_dict[str(protocol)+'_'+str(key)+'_'+str(date)+'_'+str(country)+'_'+str(domain)+'_'+str(ip)] = q2_hash\n",
    "        else:\n",
    "            upgma_dict[str(protocol)+'_S_'+str(key)+'_'+str(date)+'_'+str(country)+'_'+str(domain)+'_'+str(ip)] = q2_hash\n",
    "    for key in proto_loners: #loner ssdeep\n",
    "        target = proto_df_payload[proto_df_payload.idx == key]\n",
    "        target = target.reset_index(drop=True)\n",
    "        t_len = len(target)\n",
    "        q2 = int(t_len*0.5)\n",
    "        q2_hash = target.iloc[q2,-1]\n",
    "        country = proto_df.loc[key,'country']\n",
    "        ip = proto_df.loc[key,'ip_src']\n",
    "        domain = proto_df.loc[key,'domain']\n",
    "        upgma_dict[str(protocol)+'_L_'+str(key)+'_'+str(date)+'_'+str(country)+'_'+str(domain)+'_'+str(ip)] = q2_hash\n",
    "    return upgma_dict\n",
    "\n",
    "\n",
    "\n",
    "def pair_wise_score(upgma_dict):\n",
    "    '''\n",
    "    GOAL: calculate distance matrix by calculating paire-wise similarity score. \n",
    "    and pick upper triangle convert to list\n",
    "    Input: from cluster_rep_hash() function\n",
    "    ['cluster']: name\n",
    "    ['ssdeep']: cluster's representation hash\n",
    "    \n",
    "    Return: df=>['c_ssdeep_li']:the hashes list compare to, ['score']:list of distances (upper-triangle, exclude self)\n",
    "    '''\n",
    "    def compare(target_hash,candidate_hash_li):\n",
    "        score_li = []\n",
    "        for c_hash in candidate_hash_li:\n",
    "            score_li.append(100-ssdeep.compare(target_hash,c_hash)) #相似度滿分100，轉換成距離最近0\n",
    "        return score_li\n",
    "    used_idx = []\n",
    "    def create_hash_li(t_hash):\n",
    "        idx_set = set(upgma_df[upgma_df.ssdeep == t_hash].index)#[0]\n",
    "        same_hash_li = sorted(list(idx_set - set(used_idx)))\n",
    "        idx = same_hash_li[0]\n",
    "        used_idx.append(idx)\n",
    "        return upgma_df.loc[idx+1:]['ssdeep'].tolist()\n",
    "    upgma_df = pd.DataFrame(upgma_dict.items(),columns=['cluster','ssdeep'])\n",
    "    upgma_df['c_ssdeep_li'] = upgma_df.ssdeep.map(create_hash_li)\n",
    "    upgma_df['score'] = upgma_df.apply(lambda x: compare(x.ssdeep, x.c_ssdeep_li), axis=1)\n",
    "    return upgma_df\n",
    "\n",
    "\n",
    "\n",
    "def draw_upgma(upgma_df,picture_dir=picture_dir,name='upgma'):\n",
    "    '''\n",
    "    GOAL: using upper triangle's distance to draw upgma\n",
    "    Input: from pair_wise_score() function\n",
    "    Output: diagram of UPGMA、Z info\n",
    "    '''\n",
    "    if not os.path.exists(picture_dir):\n",
    "        os.makedirs(picture_dir,exist_ok=True) \n",
    "    score_li = upgma_df['score'].tolist()\n",
    "    score_li = list(filter(None, score_li))\n",
    "    score_li = sum(score_li,[])\n",
    "    Z = linkage(score_li, 'average')\n",
    "    fig = plt.figure(figsize=(60, 24)) #(25,10) #(5,2)\n",
    "    # plt.savefig(fig)\n",
    "    dn = dendrogram(Z,labels=upgma_df.cluster.tolist())\n",
    "    plt.savefig(picture_dir+str(name)+'.png', dpi=600, format='png', bbox_inches='tight')\n",
    "    return dn,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knee_point(length_li,proto='http',date='20200110',k=100):\n",
    "    \"\"\"\n",
    "    每個list值所包含數量的變化量，也就是點的密集度變化量 (並非list自己本身值的變化量)\n",
    "    length_li: 長度的list\n",
    "    k: slope topK's knee points\n",
    "    Return dict format (key is the answer)\n",
    "    \"\"\"\n",
    "    length_dict = dict(Counter(length_li))\n",
    "    sorted_dict = {k: v for k, v in sorted(length_dict.items(), key=lambda x: x[1])}\n",
    "    all_items_num = sum(list(length_dict.values()))\n",
    "#     for item in sorted_dict.items():\n",
    "    all_keys = list(sorted_dict.keys())\n",
    "    all_values = list(sorted_dict.values())\n",
    "    slope_li = []\n",
    "    for i in range(len(sorted_dict)):\n",
    "        length1 = all_keys[i]\n",
    "        try:\n",
    "            length2 = all_keys[i+1]\n",
    "        except IndexError:\n",
    "            break\n",
    "        value1 = sum(all_values[:i+1])\n",
    "        value2 = sum(all_values[:i+2])\n",
    "        slope = ((value2-value1)/all_items_num)/(length2-length1)\n",
    "        slope_li.append(slope)\n",
    "    change_rate_li = []\n",
    "    for i in range(len(slope_li)):\n",
    "        try:\n",
    "            slope1 = slope_li[i]\n",
    "            slope2 = slope_li[i+1]\n",
    "        except IndexError:\n",
    "            break\n",
    "        change_rate_li.append(abs(slope2-slope1)) #陡變緩或是緩變陡的都一起算\n",
    "    idx_li = sorted(range(len(change_rate_li)), key=lambda i: change_rate_li[i], reverse=True)[:k]\n",
    "    return_dict = {}\n",
    "    for idx in idx_li:\n",
    "        return_dict[all_keys[idx+1]] = change_rate_li[idx]\n",
    "    ##畫圖可再自行修改\n",
    "    temp = sorted(length_li,reverse=True)\n",
    "    x_axis = [x for x in range(len(temp))] #隨便給個編號當成X軸\n",
    "    plt.figure(figsize=(30,20),dpi=300,linewidth = 1) # 圖片大小、折線寬度\n",
    "    plt.plot(x_axis,temp,'o-',color = 'b', label=proto) #折現的型態、折現的顏色\n",
    "#     plt.title(, x=0.5, y=1.03)\n",
    "    plt.figtext(.5,.9,str(date)+\" \"+str(proto)+\" clusters' size\",fontsize=30,ha='center')\n",
    "#     plt.xticks(fontsize=20)\n",
    "#     plt.yticks(fontsize=20)\n",
    "#     plt.xlabel(\"cluster ID\", fontsize=25, labelpad = 15)\n",
    "#     plt.ylabel(\"session #\", fontsize=25, labelpad = 20)\n",
    "    # plt.ylim(0, 2500)\n",
    "    # plt.legend(loc = \"best\", fontsize=20)\n",
    "    plt.savefig(picture_dir+str(proto)+'_'+str(date)+'_clustersize_line'+'.png', dpi=300, format='png')\n",
    "#     plt.show()\n",
    "    return pd.DataFrame.from_dict(return_dict,orient='index',columns=['knee_slope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_cluster(proto_big_dict,proto_loner_num,q1_num,proto,date):\n",
    "    '''\n",
    "    proto_big_dict: big dict from similarity_compare() func.\n",
    "    proto_loner_num: the protocol's loner numbers\n",
    "    q1_num: 25%'s members# from\n",
    "    \n",
    "    Return: how many lesser is small cluster to deal as loner  (dict's member num)\n",
    "    '''\n",
    "    proto_dict_len = {}\n",
    "    for k,v in proto_big_dict.items():\n",
    "        proto_dict_len[k] = len(v)+1\n",
    "    temp = list(proto_dict_len.values())\n",
    "    for i in range(proto_loner_num):\n",
    "        temp.append(1)\n",
    "    temp = sorted(temp,reverse=True)\n",
    "    temp_df = knee_point(temp,proto=proto,date=date)\n",
    "    temp_score = temp_df.index.tolist()\n",
    "    try:\n",
    "        if temp_df.index[0] <q1_num:\n",
    "            return temp_df.index[0]-1\n",
    "        else:\n",
    "            for score in temp_score:\n",
    "                if score <q1_num:\n",
    "                    return int(score)-1 #往後找其他knee point要小於q1\n",
    "            return 0 #都不合<q1\n",
    "    except IndexError:\n",
    "        return 0 #群太少人太小分不出knee point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_cluster_info(protocol_loners,protocol_big_dict,protocol_df,pickle_dir,proto='http',df2=df2):\n",
    "    '''\n",
    "    GOAL: output loner & each cluster's three tuples pickles.\n",
    "    save file in pickle_dir+protol/ dir\n",
    "    \n",
    "    protocol_loners: from similarity_compare() func.\n",
    "    protocol_big_dict: from similarity_compare() func.\n",
    "    proto: now using what kind of protocol\n",
    "    df2: payload whole df\n",
    "    '''\n",
    "    loner_df = df2.loc[protocol_loners]\n",
    "    time_lists = loner_df.session_time_list.tolist()\n",
    "    time_lists = [list(x) for x in time_lists]\n",
    "    ips = loner_df.ip_src.tolist()\n",
    "    countries = loner_df.country.tolist()\n",
    "    pkl_dir = pickle_dir+proto+'/'\n",
    "    if not os.path.exists(pkl_dir):\n",
    "        os.makedirs(pkl_dir)\n",
    "    pickle.dump(obj=(time_lists,ips,countries),file=open(pkl_dir+'case#loners'+'_threetuples.pkl','wb'))\n",
    "    #cluster\n",
    "    protocol_big_dict = {k: v for k, v in sorted(protocol_big_dict.items(), key=lambda item: len(item[1]),reverse=True)}\n",
    "    for i,k in enumerate(protocol_big_dict.keys()):\n",
    "        write2pkl(protocol_df,protocol_big_dict,k,str(i+1)+\"_\"+str(k),path=pkl_dir)\n",
    "    return protocol_big_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run all protocols needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols_tuples = []\n",
    "def run_all(proto,date,df2,pickle_dir,picture_dir): \n",
    "    '''\n",
    "    proto: protocol contains for e.g., http\n",
    "    date: day number e.g.,20200110\n",
    "    '''\n",
    "    proto_df, proto_df_payload = prepare_df(df2,proto)\n",
    "    proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict = similarity_compare(proto_df,proto_df_payload,thr=0)\n",
    "    print(\"==========\",str(proto)+\" clusters ==========\")\n",
    "    try:\n",
    "        proto_loner_num,stat_df = cluster_stat(proto_big_dict,proto_df,proto=proto,picture_dir=picture_dir,date=date)\n",
    "        proto_big_dict = case_cluster_info(proto_loners,proto_big_dict,proto_df,pickle_dir=pickle_dir,proto=proto,df2=df2)\n",
    "        small_clu_num = get_small_cluster(proto_big_dict,proto_loner_num,stat_df.loc['25%'].values[0],proto,date)\n",
    "        proto_upgma_dict = cluster_rep_hash(proto_big_dict,proto_df_payload,proto_df,proto_loners,knee_point=small_clu_num,date=date,protocol=proto)\n",
    "        pickle.dump(obj=proto_upgma_dict,file=open(pickle_dir+str(proto)+'_upgma_dict_'+str(date)+'.pkl','wb'))\n",
    "        pickle.dump(obj=(proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                        proto_upgma_dict,stat_df),file=open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','wb'))\n",
    "    except KeyError:\n",
    "        print(\"!!!Didn't save:\",date,proto,\"!!!\")\n",
    "    plt.close()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Running date: 台灣大哥大 20200111 ===\n"
     ]
    }
   ],
   "source": [
    "#20hrs per day for all protocols\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "for da in date_li:\n",
    "    print(\"===Running date:\",isp,da,\"===\")\n",
    "    df2,pickle_dir,picture_dir = create_orifinalDF(da) #spark ram: 160G , df ram: 150G\n",
    "    dir_path = \"/mnt/Raid160TB/pcap_inter/\"+str(da[:4])+'_'+str(da[4:6])+'_'+str(da[6:])+\"/\"+isp+\"/intermeidate_data/\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path,exist_ok=True)    \n",
    "    pickle.dump(file=open(dir_path+\"df2_tuples_\"+isp+\".pkl\",'wb'),protocol=pickle.HIGHEST_PROTOCOL,\n",
    "            obj=(df2,pickle_dir,picture_dir))\n",
    "    print(da,'Done df2.')\n",
    "    argli = []\n",
    "    for protocol in protocols_need:\n",
    "        argli.append((protocol,da,df2,pickle_dir,picture_dir))\n",
    "    gc.collect()\n",
    "    with ThreadPool(3) as pool:\n",
    "        results = pool.starmap(run_all,argli)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
