{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssdeep\n",
    "import pickle,os,sys,gc\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "import time,datetime\n",
    "from itertools import chain\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import functools\n",
    "import itertools\n",
    "import operator\n",
    "from itertools import chain\n",
    "sys.setrecursionlimit(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isp = '遠傳電信'\n",
    "time_li = date_li = ['20200106','20200107','20200108','20200109','20200110','20200111','20200112']\n",
    "protocols_need = proto_li = ['http','mysql','ftp','smb','smtp','imap','pop','rpc','ssh','telnet','sip']\n",
    "time = str(min(time_li))\n",
    "picture_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(time[:4])+'_'+str(time[4:6])+'_'+str(time[6:])+'/'+isp+'/case_pictures/'\n",
    "file_name = \"_\".join(sorted(date_li))\n",
    "min_date = str(min(date_li))\n",
    "max_date = str(max(date_li))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 畫跨天protocol大小圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_dict_time_all = {}\n",
    "trend_dict_country_all = {}\n",
    "trend_dict_ip_all = {}\n",
    "trend_dict_ssdeep_all = {}\n",
    "def draw_trend_pic(date,proto,pickle_dir,trend_dict_time_all=trend_dict_time_all,trend_dict_country_all=trend_dict_country_all,trend_dict_ip_all=trend_dict_ip_all,trend_dict_ssdeep_all=trend_dict_ssdeep_all):\n",
    "    '''\n",
    "    GOAL: output dict to draw trend-circle picture\n",
    "    \n",
    "    Return: 4 dict for the same proto for the next day\n",
    "    '''\n",
    "    try:\n",
    "        (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "         proto_upgma_dict,stat_df) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "    except ValueError:\n",
    "        try:\n",
    "            (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "             proto_upgma_dict,stat_df,df2) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "        except ValueError:\n",
    "            try:\n",
    "                (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                        proto_upgma_dict,stat_df) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "            except ValueError:\n",
    "                (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                        proto_upgma_dict) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "    except FileNotFoundError:\n",
    "        print(\"!!File Not Found:\",date,proto,\"!!\")\n",
    "        return trend_dict_time_all,trend_dict_country_all,trend_dict_ip_all,trend_dict_ssdeep_all\n",
    "\n",
    "\n",
    "    def ssdeep_compare(target_hash,candidate_df):\n",
    "        '''\n",
    "        Input1: string of hash\n",
    "        Input2: dataframe of candidate\n",
    "        '''\n",
    "        def compare(candidate_hash):\n",
    "            if type(candidate_hash) == str:\n",
    "                return ssdeep.compare(target_hash,candidate_hash)\n",
    "            else:\n",
    "                score_li = []\n",
    "                for c_h in candidate_hash:\n",
    "                    score_li.append(ssdeep.compare(target_hash,c_h))\n",
    "                return max(score_li)\n",
    "        return candidate_df.hash.map(compare)\n",
    "\n",
    "    trend_dict_time = {}\n",
    "    trend_dict_country = {}\n",
    "    trend_dict_ip = {}\n",
    "    trend_dict_ssdeep = {}\n",
    "    candidate_df = pd.DataFrame(trend_dict_ssdeep_all.items(),columns=['idx','hash']) #其他已經有的cluster\n",
    "    for key,value in proto_big_dict.items():\n",
    "        target = proto_df_payload[proto_df_payload.idx == key]\n",
    "        target = target.reset_index(drop=True)\n",
    "        t_len = len(target)\n",
    "        q2 = int(t_len*0.5)\n",
    "        t_q2_key = target.iloc[q2,-1] #要跟別人比較的cluster hash\n",
    "        if trend_dict_ssdeep_all == {}: #第一次\n",
    "            member_li = value[:]\n",
    "            \n",
    "            t_q2_member1 = proto_df_payload[proto_df_payload.idx == member_li[-1]] # member list最後一個\n",
    "            t_q2_member1 = t_q2_member1.reset_index(drop=True)\n",
    "            t_len = len(t_q2_member1)\n",
    "            q2 = int(t_len*0.5)\n",
    "            t_q2_member1 = t_q2_member1.iloc[q2,-1]\n",
    "            \n",
    "            t_q2_member2 = proto_df_payload[proto_df_payload.idx == max(member_li)] #member list最大的那個\n",
    "            t_q2_member2 = t_q2_member2.reset_index(drop=True)\n",
    "            t_len = len(t_q2_member2)\n",
    "            q2 = int(t_len*0.5)\n",
    "            t_q2_member2 = t_q2_member2.iloc[q2,-1]\n",
    "            \n",
    "            trend_dict_ssdeep[str(date)+\"_\"+str(key)] = [t_q2_key,t_q2_member1,t_q2_member2] #可以增加hash candidate?\n",
    "            member_li.append(key)\n",
    "            select_df = proto_df.loc[member_li]\n",
    "            time_li = select_df.session_time.tolist()\n",
    "            trend_dict_time[str(date)+\"_\"+str(key)] = time_li\n",
    "            country_li = select_df.country.tolist()\n",
    "            ip_li = select_df.ip_src.tolist()\n",
    "            assert len(time_li) == len(country_li) == len(ip_li)\n",
    "            trend_dict_country[str(date)+\"_\"+str(key)] = country_li\n",
    "            trend_dict_ip[str(date)+\"_\"+str(key)] = ip_li\n",
    "        else: #後面幾次\n",
    "            candidate_df['score'] = ssdeep_compare(t_q2_key,candidate_df) \n",
    "            max_score = candidate_df.score.max()\n",
    "            if max_score < 1: #都沒有相近的\n",
    "                \n",
    "                member_li = value[:]\n",
    "                \n",
    "                t_q2_member1 = proto_df_payload[proto_df_payload.idx == member_li[-1]] # member list最後一個\n",
    "                t_q2_member1 = t_q2_member1.reset_index(drop=True)\n",
    "                t_len = len(t_q2_member1)\n",
    "                q2 = int(t_len*0.5)\n",
    "                t_q2_member1 = t_q2_member1.iloc[q2,-1]\n",
    "\n",
    "                t_q2_member2 = proto_df_payload[proto_df_payload.idx == max(member_li)] #member list最大的那個\n",
    "                t_q2_member2 = t_q2_member2.reset_index(drop=True)\n",
    "                t_len = len(t_q2_member2)\n",
    "                q2 = int(t_len*0.5)\n",
    "                t_q2_member2 = t_q2_member2.iloc[q2,-1]\n",
    "                \n",
    "                trend_dict_ssdeep[str(date)+\"_\"+str(key)] = [t_q2_key,t_q2_member1,t_q2_member2]\n",
    "                member_li.append(key)\n",
    "                select_df = proto_df.loc[member_li]\n",
    "                time_li = select_df.session_time.tolist()\n",
    "                trend_dict_time[str(date)+\"_\"+str(key)] = time_li\n",
    "                country_li = select_df.country.tolist()\n",
    "                ip_li = select_df.ip_src.tolist()\n",
    "                assert len(time_li) == len(country_li) == len(ip_li)\n",
    "                trend_dict_country[str(date)+\"_\"+str(key)] = country_li\n",
    "                trend_dict_ip[str(date)+\"_\"+str(key)] = ip_li\n",
    "            elif max_score>0: #有相近的合併到原本的dict\n",
    "                try:\n",
    "                    combine_id = candidate_df[candidate_df.score == max_score].idx.tolist()[0]\n",
    "                except:\n",
    "                    print( candidate_df,max_score,target)\n",
    "                ori_ssdeep_list = trend_dict_ssdeep_all[combine_id][:]\n",
    "                if type(ori_ssdeep_list) == str:\n",
    "                    ori_ssdeep_list = [ori_ssdeep_list] #之前只有一個ssdeep hash\n",
    "                ori_time_list = trend_dict_time_all[combine_id][:]\n",
    "                ori_country_list = trend_dict_country_all[combine_id][:]\n",
    "                ori_ip_list = trend_dict_ip_all[combine_id][:]\n",
    "                member_li = value[:]\n",
    "                \n",
    "                t_q2_member1 = proto_df_payload[proto_df_payload.idx == member_li[-1]] # member list最後一個\n",
    "                t_q2_member1 = t_q2_member1.reset_index(drop=True)\n",
    "                t_len = len(t_q2_member1)\n",
    "                q2 = int(t_len*0.5)\n",
    "                t_q2_member1 = t_q2_member1.iloc[q2,-1]\n",
    "\n",
    "                t_q2_member2 = proto_df_payload[proto_df_payload.idx == max(member_li)] #member list最大的那個\n",
    "                t_q2_member2 = t_q2_member2.reset_index(drop=True)\n",
    "                t_len = len(t_q2_member2)\n",
    "                q2 = int(t_len*0.5)\n",
    "                t_q2_member2 = t_q2_member2.iloc[q2,-1]                \n",
    "                \n",
    "                t_q2 = [t_q2_key,t_q2_member1,t_q2_member2]\n",
    "                member_li.append(key)\n",
    "                select_df = proto_df.loc[member_li]\n",
    "                time_li = select_df.session_time.tolist()\n",
    "                ori_time_list.extend(time_li)\n",
    "                country_li = select_df.country.tolist()\n",
    "                ip_li = select_df.ip_src.tolist()\n",
    "                ori_country_list.extend(country_li)\n",
    "                ori_ip_list.extend(ip_li)\n",
    "                ori_ssdeep_list.extend(t_q2) #我合併進入別人的群，所以把我群的key hash也加入\n",
    "                assert len(ori_time_list) == len(ori_country_list) == len(ori_ip_list)\n",
    "                trend_dict_time_all[combine_id] = ori_time_list\n",
    "                trend_dict_country_all[combine_id] = ori_country_list\n",
    "                trend_dict_ip_all[combine_id] = ori_ip_list\n",
    "                trend_dict_ssdeep_all[combine_id] = ori_ssdeep_list\n",
    "            else:\n",
    "                print(max_score)\n",
    "    trend_dict_time_all.update(trend_dict_time)\n",
    "    trend_dict_country_all.update(trend_dict_country)\n",
    "    trend_dict_ip_all.update(trend_dict_ip)\n",
    "    trend_dict_ssdeep_all.update(trend_dict_ssdeep)\n",
    "    return trend_dict_time_all,trend_dict_country_all,trend_dict_ip_all,trend_dict_ssdeep_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_li(time_li, country_li):\n",
    "    '''\n",
    "    GOAL: sort by time (align with time's order)\n",
    "    Return: list\n",
    "    '''\n",
    "    sort_country_li = [x for _,x in sorted(zip(time_li,country_li))]\n",
    "    return sort_country_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for proto in tqdm(proto_li):\n",
    "    trend_dict_time_all = {}\n",
    "    trend_dict_country_all = {}\n",
    "    trend_dict_ip_all = {}\n",
    "    trend_dict_ssdeep_all = {}\n",
    "    for date in date_li:\n",
    "        pickle_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(date[:4])+'_'+str(date[4:6])+'_'+str(date[6:])+'/'+isp+'/case_pickles/'\n",
    "        trend_dict_time_all,trend_dict_country_all,trend_dict_ip_all,trend_dict_ssdeep_all = draw_trend_pic(date,proto,\n",
    "                                                                                          pickle_dir,trend_dict_time_all,trend_dict_country_all,trend_dict_ip_all,trend_dict_ssdeep_all)\n",
    "    trend_dict_time_all = {k: v for k, v in sorted(trend_dict_time_all.items(), key=lambda item: len(item[1]),reverse=True)}\n",
    "    time_df = pd.DataFrame(trend_dict_time_all.items(),columns=['idx','timestamp'])\n",
    "    trend_dict_country_all = {k: v for k, v in sorted(trend_dict_country_all.items(), key=lambda item: len(item[1]),reverse=True)}\n",
    "    country_df = pd.DataFrame(trend_dict_country_all.items(),columns=['idx','country'])\n",
    "    trend_dict_ip_all = {k: v for k, v in sorted(trend_dict_ip_all.items(), key=lambda item: len(item[1]),reverse=True)}\n",
    "    ip_df =  pd.DataFrame(trend_dict_ip_all.items(),columns=['idx','src_ip'])\n",
    "    ssdeep_df = pd.DataFrame(trend_dict_ssdeep_all.items(),columns=['idx','ssdeep'])\n",
    "    all_df = pd.merge(time_df,country_df,on='idx')\n",
    "    all_df =  pd.merge(all_df,ip_df,on='idx')\n",
    "    all_df = pd.merge(all_df,ssdeep_df,on='idx')\n",
    "    all_df['country'] = all_df.apply(lambda x: sort_li(x.timestamp, x.country), axis=1)\n",
    "    all_df['src_ip'] = all_df.apply(lambda x: sort_li(x.timestamp, x.src_ip), axis=1)\n",
    "    all_df['timestamp'] = all_df.timestamp.map(sorted)\n",
    "    file_name = \"_\".join(sorted(date_li))\n",
    "    date_li2 = [int(x) for x in date_li]\n",
    "    min_date = str(min(date_li2))\n",
    "    pickle.dump(obj=all_df,\n",
    "                file=open('/home/antslab/NAS1_RAID6/pcap_inter/'+str(min_date[:4])+'_'+str(min_date[4:6])+'_'+str(min_date[6:])+'/'+isp+'/case_pickles/'+proto+'_trend_df_'+file_name+'.pkl','wb'))\n",
    "    print(\"Protocol Pattern draw save path:\",'/home/antslab/NAS1_RAID6/pcap_inter/'+str(min_date[:4])+'_'+str(min_date[4:6])+'_'+str(min_date[6:])+'/'+isp+'/case_pickles/'+proto+'_trend_df_'+file_name+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "輸出cluster之key session的time list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for proto in tqdm(proto_li):\n",
    "    http_df = pickle.load(open('/home/antslab/NAS1_RAID6/pcap_inter/'+str(min_date[:4])+'_'+str(min_date[4:6])+'_'+str(min_date[6:])+'/'+isp+'/case_pickles/'+proto+'_trend_df_'+file_name+'.pkl','rb'))\n",
    "\n",
    "    #輸出cluster之key session的time list\n",
    "    wireshark_li = http_df.idx.tolist() #.head(15)前15大cluster #改!proto\n",
    "    wireshark_rank = []\n",
    "    for i,v in enumerate(wireshark_li):\n",
    "        wireshark_rank.append(i+1)\n",
    "    wireshark_rank = [x for _,x in sorted(zip(wireshark_li,wireshark_rank))]\n",
    "    wireshark_li = sorted(wireshark_li)\n",
    "\n",
    "    save_path_li = []\n",
    "    now_date = '00' #現在正在處理的日期\n",
    "    for i,wireshark in zip(wireshark_rank,wireshark_li):\n",
    "        date = wireshark.split('_')[0] #該cluster key的同月份日期\n",
    "        if date!= now_date: #新日期才要重讀\n",
    "            now_date = date\n",
    "\n",
    "            pickle_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(date[:4])+'_'+str(date[4:6])+'_'+str(date[6:])+'/'+isp+'/case_pickles/'\n",
    "            try:\n",
    "                (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                 proto_upgma_dict,stat_df,df2) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                            proto_upgma_dict,stat_df) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "                except ValueError:\n",
    "                    (proto_df, proto_df_payload,proto_big_dict,proto_loners,proto_score,proto_cluster_score_dict,\n",
    "                                            proto_upgma_dict) = pickle.load(open(pickle_dir+str(date)+'_'+str(proto)+'_all.pkl','rb'))\n",
    "            except FileNotFoundError:\n",
    "                print(\"!!File Not Found:\",date,proto,\"!!\")\n",
    "        idx = wireshark.split('_')[-1] #該cluster在該日期該proto的df中的index\n",
    "        #     else:\n",
    "        try:\n",
    "            time_list = proto_df.loc[int(idx),'session_time_list'].tolist()\n",
    "        except AttributeError:\n",
    "            time_list = proto_df.loc[int(idx),'session_time_list']\n",
    "        timelist_path = pickle_dir+'timelist_'+proto+'_large#'+str(i)+'_clusterID#'+str(idx)+'.pkl'\n",
    "        pickle.dump(file=open(timelist_path,'wb'),obj=time_list)\n",
    "#         print(wireshark,'save in:',timelist_path) #truly save path\n",
    "        save_path_li.append(timelist_path)\n",
    "    try:\n",
    "        first_date = wireshark_li[0].split('_')[0]#[-2:]\n",
    "        save_path = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(first_date[:4])+'_'+str(first_date[4:6])+'_'+str(first_date[6:])+'/'+isp+'/case_pickles/'+proto+'_clusterKey_timelist_paths_'+file_name+'.pkl'\n",
    "        pickle.dump(file=open(save_path,'wb'),obj=save_path_li)  \n",
    "        print(proto,':',save_path) #for 証鴻 pickle save path\n",
    "    except IndexError:\n",
    "        print(proto,\"Not Save, because it's empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 新關聯方法\n",
    "* topology\n",
    "    * 我們會先找所給定期間的指定所有protocols之所有sessions與對應IPs\n",
    "    * 接下來會利用此段期間的各IP，去尋找這個IP在這段期間做的手法(攻擊樣態群集)\n",
    "    * 找出不同IP所橫跨對應的攻擊樣態群，計算jaccard相似度\n",
    "    * 將所採用相似手法(score>thr)的IP群聚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i,proto in enumerate(proto_li):\n",
    "    if i==0:\n",
    "        all_df = pickle.load(file=open('/home/antslab/NAS1_RAID6/pcap_inter/'+str(min_date[:4])+'_'+str(min_date[4:6])+'_'+str(min_date[6:])+'/'+isp+'/case_pickles/'+proto+'_trend_df_'+file_name+'.pkl','rb')) #改!!)\n",
    "        all_df['idx'] = all_df['idx']+'_'+proto\n",
    "    else:\n",
    "        temp = pickle.load(file=open('/home/antslab/NAS1_RAID6/pcap_inter/'+str(min_date[:4])+'_'+str(min_date[4:6])+'_'+str(min_date[6:])+'/'+isp+'/case_pickles/'+proto+'_trend_df_'+file_name+'.pkl','rb')) #改!!\n",
    "        temp['idx'] = temp['idx']+'_'+proto\n",
    "        all_df = all_df.append(temp)\n",
    "all_df = all_df.reset_index(drop=True)\n",
    "#首次須先輸出noise pkl給專家，另外處理後才會獲得noise_clusters\n",
    "pickle_dir = '/home/antslab/NAS1_RAID6/pcap_inter/'+str(min_date[:4])+'_'+str(min_date[4:6])+'_'+str(min_date[6:])+'/'+isp+'/case_pickles/'\n",
    "pickle.dump(file=open(pickle_dir+'clusterName_overview_denoise_df_'+str(min_date)+'_'+str(date_li[-1])+'.pkl','wb'),obj=all_df)\n",
    "print(\"Denoise path save in:\",\n",
    "      pickle_dir+'clusterName_overview_denoise_df_'+str(min_date)+'_'+str(date_li[-1])+'.pkl')\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
