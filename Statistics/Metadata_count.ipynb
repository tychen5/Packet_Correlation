{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  count file for statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import random\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"../0112_0125/\"\n",
    "hp_date_first = False #先分日期才分四種hp?\n",
    "pcap_date_dir = True #pcap資料夾有多一層日期資料夾?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020_01_12',\n",
       " '2020_01_13',\n",
       " '2020_01_14',\n",
       " '2020_01_15',\n",
       " '2020_01_16',\n",
       " '2020_01_17',\n",
       " '2020_01_18',\n",
       " '2020_01_19',\n",
       " '2020_01_20',\n",
       " '2020_01_21',\n",
       " '2020_01_22',\n",
       " '2020_01_23',\n",
       " '2020_01_24',\n",
       " '2020_01_25']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isps = next(os.walk(root_dir))[1]\n",
    "if 'csv' in isps:\n",
    "    isps.remove('csv')\n",
    "if 'meta_data' in isps:\n",
    "    isps.remove('meta_data')\n",
    "# pcap date\n",
    "all_dates = []\n",
    "for isp in isps:\n",
    "#     ori_dirs = next(os.walk(root_dir + isp))[1] #honeypot/pcap\n",
    "    pcap_dir = root_dir + isp + '/pcap/'\n",
    "    hp_dir = root_dir + isp + '/honeypot/'\n",
    "    if pcap_date_dir == False:\n",
    "        pcap_files = next(os.walk(pcap_dir))[2]\n",
    "        pcap_files = list(filter(lambda f: f.endswith(\".tar.gz\"), pcap_files))\n",
    "        dates = [x.split('.')[1] for x in pcap_files]\n",
    "        dates = [x.replace('-','_') for x in dates]\n",
    "        all_dates.extend(dates)\n",
    "    else:\n",
    "        pcap_dates = next(os.walk(pcap_dir))[1] #csv/meta_data need to be excluded\n",
    "        all_dates.extend(pcap_dates)\n",
    "    if hp_date_first:\n",
    "        hp_dates = next(os.walk(hp_dir))[1]\n",
    "        dates = [x.replace('-','_') for x in hp_dates]\n",
    "        all_dates.extend(dates)\n",
    "    else:\n",
    "        hp_types_dir = next(os.walk(hp_dir))[1]\n",
    "        for hp_type in hp_types_dir:\n",
    "            hp_dates = next(os.walk(hp_dir + hp_type + '/'))[1]\n",
    "            all_dates.extend(hp_dates)\n",
    "        \n",
    "#     for o_d in ori_dirs:\n",
    "all_dates = sorted(list(set(all_dates)))\n",
    "all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(obj=all_dates,file=open(root_dir+'dates.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISP</th>\n",
       "      <th>pcap_loss_date</th>\n",
       "      <th>amun_loss_date</th>\n",
       "      <th>cowrie_loss_date</th>\n",
       "      <th>dionaea_loss_date</th>\n",
       "      <th>glastopf_loss_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>中華電信</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中嘉寬頻</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>台固媒體</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2020_01_17</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>台灣大哥大</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>台灣之星</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>台灣固網</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>台灣基礎開發</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2020_01_12、2020_01_13、2020_01_16、2020_01_22、20...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>台灣碩網</td>\n",
       "      <td></td>\n",
       "      <td>2020_01_13、2020_01_25</td>\n",
       "      <td>2020_01_25</td>\n",
       "      <td>2020_01_25</td>\n",
       "      <td>2020_01_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>亞太電信</td>\n",
       "      <td></td>\n",
       "      <td>2020_01_25</td>\n",
       "      <td>2020_01_25</td>\n",
       "      <td>2020_01_25</td>\n",
       "      <td>2020_01_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>凱擘</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>遠傳</td>\n",
       "      <td>2020_01_12、2020_01_13、2020_01_14</td>\n",
       "      <td></td>\n",
       "      <td>2020_01_21</td>\n",
       "      <td></td>\n",
       "      <td>2020_01_12、2020_01_13、2020_01_14、2020_01_15、20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ISP                    pcap_loss_date         amun_loss_date  \\\n",
       "0     中華電信                                                            \n",
       "1     中嘉寬頻                                                            \n",
       "2     台固媒體                                                            \n",
       "3    台灣大哥大                                                            \n",
       "4     台灣之星                                                            \n",
       "5     台灣固網                                                            \n",
       "6   台灣基礎開發                                                            \n",
       "7     台灣碩網                                    2020_01_13、2020_01_25   \n",
       "8     亞太電信                                               2020_01_25   \n",
       "9       凱擘                                                            \n",
       "10      遠傳  2020_01_12、2020_01_13、2020_01_14                          \n",
       "\n",
       "                                     cowrie_loss_date dionaea_loss_date  \\\n",
       "0                                                                         \n",
       "1                                                                         \n",
       "2                                          2020_01_17                     \n",
       "3                                                                         \n",
       "4                                                                         \n",
       "5                                                                         \n",
       "6   2020_01_12、2020_01_13、2020_01_16、2020_01_22、20...                     \n",
       "7                                          2020_01_25        2020_01_25   \n",
       "8                                          2020_01_25        2020_01_25   \n",
       "9                                                                         \n",
       "10                                         2020_01_21                     \n",
       "\n",
       "                                   glastopf_loss_date  \n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "5                                                      \n",
       "6                                                      \n",
       "7                                          2020_01_25  \n",
       "8                                          2020_01_25  \n",
       "9                                                      \n",
       "10  2020_01_12、2020_01_13、2020_01_14、2020_01_15、20...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['ISP', 'pcap_loss_date',\n",
    "                           'amun_loss_date','cowrie_loss_date','dionaea_loss_date','glastopf_loss_date'])\n",
    "for isp in isps:\n",
    "    pcap_dir = root_dir + isp + '/pcap/'\n",
    "    hp_dir = root_dir + isp + '/honeypot/'\n",
    "    pcap_loss = []\n",
    "    amun_loss = []\n",
    "    cowire_loss = []\n",
    "    dionaea_loss = []\n",
    "    glastopf_loss = []\n",
    "    if pcap_date_dir == False:\n",
    "        pcap_files = list(filter(lambda f: f.endswith(\".tar.gz\"), pcap_files))\n",
    "        dates = [x.split('.')[1] for x in pcap_files]\n",
    "        dates = [x.replace('-','_') for x in dates]\n",
    "        for a_date in all_dates:\n",
    "            if a_date not in dates:\n",
    "                pcap_loss.append(a_date)\n",
    "    else:\n",
    "        pcap_dates = next(os.walk(pcap_dir))[1]\n",
    "        for a_date in all_dates:\n",
    "            if a_date not in pcap_dates:\n",
    "                pcap_loss.append(a_date)\n",
    "        for date in pcap_dates:\n",
    "            if len(os.listdir(pcap_dir+date) ) == 0:\n",
    "                pcap_loss.append(date)\n",
    "    if hp_date_first:\n",
    "        hp_dates = next(os.walk(hp_dir))[1]\n",
    "        dates = [x.replace('-','_') for x in hp_dates]\n",
    "        for a_date in all_dates:\n",
    "            if a_date not in dates:\n",
    "#                 print(isp)\n",
    "                amun_loss.append(a_date)\n",
    "                cowire_loss.append(a_date)                        \n",
    "                dionaea_loss.append(a_date)                        \n",
    "                glastopf_loss.append(a_date)\n",
    "        for date in hp_dates:\n",
    "            try:\n",
    "                if len(os.listdir(hp_dir+date+'/amun') ) == 0:\n",
    "                    amun_loss.append(date)\n",
    "            except FileNotFoundError:\n",
    "                amun_loss.append(date)\n",
    "            try:\n",
    "                if len(os.listdir(hp_dir+date+'/cowrie') ) == 0:\n",
    "                    cowire_loss.append(date)\n",
    "            except FileNotFoundError:\n",
    "                cowire_loss.append(date)\n",
    "            try:\n",
    "                if len(os.listdir(hp_dir+date+'/dionaea') ) == 0:\n",
    "                    dionaea_loss.append(date)\n",
    "            except FileNotFoundError:\n",
    "                dionaea_loss.append(date)\n",
    "            try:\n",
    "                if len(os.listdir(hp_dir+date+'/glastopf') ) == 0:\n",
    "                    glastopf_loss.append(date)\n",
    "            except FileNotFoundError:\n",
    "                glastopf_loss.append(date)\n",
    "    else:\n",
    "        for a_date in all_dates:\n",
    "            try:\n",
    "                if len(os.listdir(hp_dir+'amun/'+a_date) ) == 0:\n",
    "                    amun_loss.append(a_date)\n",
    "            except FileNotFoundError:\n",
    "                amun_loss.append(a_date)\n",
    "            try:\n",
    "                if len(os.listdir(hp_dir+'cowrie/'+a_date) ) == 0:\n",
    "                    cowire_loss.append(a_date)\n",
    "            except FileNotFoundError:\n",
    "                cowire_loss.append(a_date)\n",
    "            try:\n",
    "                if len(os.listdir(hp_dir+'dionaea/'+a_date) ) == 0:\n",
    "                    dionaea_loss.append(a_date)\n",
    "            except FileNotFoundError:\n",
    "                dionaea_loss.append(a_date)\n",
    "            try:\n",
    "                if len(os.listdir(hp_dir+'glastopf/'+a_date) ) == 0:\n",
    "                    glastopf_loss.append(a_date)\n",
    "            except FileNotFoundError:\n",
    "                glastopf_loss.append(a_date)\n",
    "    temp = [isp, \"、\".join(pcap_loss), \n",
    "            \"、\".join(amun_loss),\"、\".join(cowire_loss),\"、\".join(dionaea_loss),\"、\".join(glastopf_loss)]\n",
    "    temp = pd.Series(temp,index=df.columns)\n",
    "    df = df.append(temp, ignore_index=True)\n",
    "                \n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(obj=df,file=open(root_dir+'lost_dates.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
